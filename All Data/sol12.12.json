[
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### a.\n",
     "_Exercise: The `build()` method should define two trainable weights *α* and *β*, both of shape `input_shape[-1:]` and data type `tf.float32`. *α* should be initialized with 1s, and *β* with 0s._"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Solution: see below."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### b.\n",
     "_Exercise: The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean μ and the variance σ<sup>2</sup> of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return *α*⊗(*X* - μ)/(σ + ε) + *β*, where ⊗ represents itemwise multiplication (`*`) and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001)._"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 266,
    "metadata": {},
    "outputs": [],
    "source": [
     "class LayerNormalization(keras.layers.Layer):\n",
     "    def __init__(self, eps=0.001, **kwargs):\n",
     "        super().__init__(**kwargs)\n",
     "        self.eps = eps\n",
     "\n",
     "    def build(self, batch_input_shape):\n",
     "        self.alpha = self.add_weight(\n",
     "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
     "            initializer=\"ones\")\n",
     "        self.beta = self.add_weight(\n",
     "            name=\"beta\", shape=batch_input_shape[-1:],\n",
     "            initializer=\"zeros\")\n",
     "        super().build(batch_input_shape) # must be at the end\n",
     "\n",
     "    def call(self, X):\n",
     "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
     "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
     "\n",
     "    def compute_output_shape(self, batch_input_shape):\n",
     "        return batch_input_shape\n",
     "\n",
     "    def get_config(self):\n",
     "        base_config = super().get_config()\n",
     "        return {**base_config, \"eps\": self.eps}"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Note that making _ε_ a hyperparameter (`eps`) was not compulsory. Also note that it's preferable to compute `tf.sqrt(variance + self.eps)` rather than `tf.sqrt(variance) + self.eps`. Indeed, the derivative of sqrt(z) is undefined when z=0, so training will bomb whenever the variance vector has at least one component equal to 0. Adding _ε_ within the square root guarantees that this will never happen."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### c.\n",
     "_Exercise: Ensure that your custom layer produces the same (or very nearly the same) output as the `keras.layers.LayerNormalization` layer._"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's create one instance of each class, apply them to some data (e.g., the training set), and ensure that the difference is negligeable."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 267,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(), dtype=float32, numpy=5.6045884e-08>"
       ]
      },
      "execution_count": 267,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "X = X_train.astype(np.float32)\n",
     "\n",
     "custom_layer_norm = LayerNormalization()\n",
     "keras_layer_norm = keras.layers.LayerNormalization()\n",
     "\n",
     "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
     "    keras_layer_norm(X), custom_layer_norm(X)))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Yep, that's close enough. To be extra sure, let's make alpha and beta completely random and compare again:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 268,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(), dtype=float32, numpy=2.2921004e-08>"
       ]
      },
      "execution_count": 268,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "random_alpha = np.random.rand(X.shape[-1])\n",
     "random_beta = np.random.rand(X.shape[-1])\n",
     "\n",
     "custom_layer_norm.set_weights([random_alpha, random_beta])\n",
     "keras_layer_norm.set_weights([random_alpha, random_beta])\n",
     "\n",
     "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
     "    keras_layer_norm(X), custom_layer_norm(X)))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Still a negligeable difference! Our custom layer works fine."
    ]
   }
 ]