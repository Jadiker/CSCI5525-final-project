[
{
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 10.\n",
     "_Exercise: In this exercise you will download a dataset, split it, create a `tf.data.Dataset` to load it and preprocess it efficiently, then build and train a binary classification model containing an `Embedding` layer._\n",
     "\n",
     "### a.\n",
     "_Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/). The data is organized in two directories, `train` and `test`, each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise._"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 131,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
       "84131840/84125825 [==============================] - 12s 0us/step\n"
      ]
     },
     {
      "data": {
       "text/plain": [
        "PosixPath('/Users/ageron/.keras/datasets/aclImdb')"
       ]
      },
      "execution_count": 131,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "from pathlib import Path\n",
     "\n",
     "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
     "FILENAME = \"aclImdb_v1.tar.gz\"\n",
     "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
     "path = Path(filepath).parent / \"aclImdb\"\n",
     "path"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 132,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "aclImdb/\n",
       "    README\n",
       "    imdb.vocab\n",
       "    imdbEr.txt\n",
       "    test/\n",
       "        labeledBow.feat\n",
       "        urls_neg.txt\n",
       "        urls_pos.txt\n",
       "        neg/\n",
       "            0_2.txt\n",
       "            10000_4.txt\n",
       "            10001_1.txt\n",
       "            ...\n",
       "        pos/\n",
       "            0_10.txt\n",
       "            10000_7.txt\n",
       "            10001_9.txt\n",
       "            ...\n",
       "    train/\n",
       "        labeledBow.feat\n",
       "        unsupBow.feat\n",
       "        urls_neg.txt\n",
       "        ...\n",
       "        neg/\n",
       "            0_3.txt\n",
       "            10000_4.txt\n",
       "            10001_4.txt\n",
       "            ...\n",
       "        unsup/\n",
       "            0_0.txt\n",
       "            10000_0.txt\n",
       "            10001_0.txt\n",
       "            ...\n",
       "        pos/\n",
       "            0_9.txt\n",
       "            10000_8.txt\n",
       "            10001_10.txt\n",
       "            ...\n"
      ]
     }
    ],
    "source": [
     "for name, subdirs, files in os.walk(path):\n",
     "    indent = len(Path(name).parts) - len(path.parts)\n",
     "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
     "    for index, filename in enumerate(sorted(files)):\n",
     "        if index == 3:\n",
     "            print(\"    \" * (indent + 1) + \"...\")\n",
     "            break\n",
     "        print(\"    \" * (indent + 1) + filename)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 133,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "(12500, 12500, 12500, 12500)"
       ]
      },
      "execution_count": 133,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def review_paths(dirpath):\n",
     "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
     "\n",
     "train_pos = review_paths(path / \"train\" / \"pos\")\n",
     "train_neg = review_paths(path / \"train\" / \"neg\")\n",
     "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
     "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
     "\n",
     "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### b.\n",
     "_Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 134,
    "metadata": {},
    "outputs": [],
    "source": [
     "np.random.shuffle(test_valid_pos)\n",
     "\n",
     "test_pos = test_valid_pos[:5000]\n",
     "test_neg = test_valid_neg[:5000]\n",
     "valid_pos = test_valid_pos[5000:]\n",
     "valid_neg = test_valid_neg[5000:]"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### c.\n",
     "_Exercise: Use tf.data to create an efficient dataset for each set._"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Since the dataset fits in memory, we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 135,
    "metadata": {},
    "outputs": [],
    "source": [
     "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
     "    reviews = []\n",
     "    labels = []\n",
     "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
     "        for filepath in filepaths:\n",
     "            with open(filepath) as review_file:\n",
     "                reviews.append(review_file.read())\n",
     "            labels.append(label)\n",
     "    return tf.data.Dataset.from_tensor_slices(\n",
     "        (tf.constant(reviews), tf.constant(labels)))"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 136,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "tf.Tensor(b\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", shape=(), dtype=string)\n",
       "tf.Tensor(0, shape=(), dtype=int32)\n",
       "\n",
       "tf.Tensor(b'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.', shape=(), dtype=string)\n",
       "tf.Tensor(0, shape=(), dtype=int32)\n",
       "\n",
       "tf.Tensor(b\"Ouch! This one was a bit painful to sit through. It has a cute and amusing premise, but it all goes to hell from there. Matthew Modine is almost always pedestrian and annoying, and he does not disappoint in this one. Deborah Kara Unger and John Neville turned in surprisingly decent performances. Alan Bates and Jennifer Tilly, among others, played it way over the top. I know that's the way the parts were written, and it's hard to blame actors, when the script and director have them do such schlock. If you're going to have outrageous characters, that's OK, but you gotta have good material to make it work. It didn't here. Run away screaming from this movie if at all possible.\", shape=(), dtype=string)\n",
       "tf.Tensor(0, shape=(), dtype=int32)\n",
       "\n"
      ]
     }
    ],
    "source": [
     "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
     "    print(X)\n",
     "    print(y)\n",
     "    print()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 137,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "17.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
      ]
     }
    ],
    "source": [
     "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "It takes about 17 seconds to load the dataset and go through it 10 times."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "But let's pretend the dataset does not fit in memory, just to make things more interesting. Luckily, each review fits on just one line (they use `<br />` to indicate line breaks), so we can read the reviews using a `TextLineDataset`. If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords). For very large datasets, it would make sense to use a tool like Apache Beam for that."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 138,
    "metadata": {},
    "outputs": [],
    "source": [
     "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
     "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
     "                                          num_parallel_reads=n_read_threads)\n",
     "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
     "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
     "                                          num_parallel_reads=n_read_threads)\n",
     "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
     "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 139,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "33 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
      ]
     }
    ],
    "source": [
     "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now it takes about 33 seconds to go through the dataset 10 times. That's much slower, essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch. If you add `.cache()` just before `.repeat(10)`, you will see that this implementation will be about as fast as the previous one."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 140,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "16.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
      ]
     }
    ],
    "source": [
     "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 141,
    "metadata": {},
    "outputs": [],
    "source": [
     "batch_size = 32\n",
     "\n",
     "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
     "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
     "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### d.\n",
     "_Exercise: Create a binary classification model, using a `TextVectorization` layer to preprocess each review. If the `TextVectorization` layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package, for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces, and `split()` to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's first write a function to preprocess the reviews, cropping them to 300 characters, converting them to lower case, then replacing `<br />` and all non-letter characters to spaces, splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 142,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
        "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
        "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
        "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
        "        b'<pad>']], dtype=object)>"
       ]
      },
      "execution_count": 142,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def preprocess(X_batch, n_words=50):\n",
     "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
     "    Z = tf.strings.substr(X_batch, 0, 300)\n",
     "    Z = tf.strings.lower(Z)\n",
     "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
     "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
     "    Z = tf.strings.split(Z)\n",
     "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
     "\n",
     "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
     "preprocess(X_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now let's write a second utility function that will take a data sample with the same format as the output of the `preprocess()` function, and will output the list of the top `max_size` most frequent words, ensuring that the padding token is first:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 143,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "[b'<pad>',\n",
        " b'it',\n",
        " b'great',\n",
        " b's',\n",
        " b'a',\n",
        " b'movie',\n",
        " b'i',\n",
        " b'loved',\n",
        " b'was',\n",
        " b'terrible',\n",
        " b'run',\n",
        " b'away']"
       ]
      },
      "execution_count": 143,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "from collections import Counter\n",
     "\n",
     "def get_vocabulary(data_sample, max_size=1000):\n",
     "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
     "    counter = Counter()\n",
     "    for words in preprocessed_reviews:\n",
     "        for word in words:\n",
     "            if word != b\"<pad>\":\n",
     "                counter[word] += 1\n",
     "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
     "\n",
     "get_vocabulary(X_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now we are ready to create the `TextVectorization` layer. Its constructor just saves the hyperparameters (`max_vocabulary_size` and `n_oov_buckets`). The `adapt()` method computes the vocabulary using the `get_vocabulary()` function, then it builds a `StaticVocabularyTable` (see Chapter 16 for more details). The `call()` method preprocesses the reviews to get a padded list of words for each review, then it uses the `StaticVocabularyTable` to lookup the index of each word in the vocabulary:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 144,
    "metadata": {},
    "outputs": [],
    "source": [
     "class TextVectorization(keras.layers.Layer):\n",
     "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
     "        super().__init__(dtype=dtype, **kwargs)\n",
     "        self.max_vocabulary_size = max_vocabulary_size\n",
     "        self.n_oov_buckets = n_oov_buckets\n",
     "\n",
     "    def adapt(self, data_sample):\n",
     "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
     "        words = tf.constant(self.vocab)\n",
     "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
     "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
     "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
     "        \n",
     "    def call(self, inputs):\n",
     "        preprocessed_inputs = preprocess(inputs)\n",
     "        return self.table.lookup(preprocessed_inputs)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's try it on our small `X_example` we defined earlier:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 145,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
        "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0],\n",
        "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
        "         0,  0]])>"
       ]
      },
      "execution_count": 145,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "text_vectorization = TextVectorization()\n",
     "\n",
     "text_vectorization.adapt(X_example)\n",
     "text_vectorization(X_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Looks good! As you can see, each review was cleaned up and tokenized, then each word was encoded as its index in the vocabulary (all the 0s correspond to the `<pad>` tokens).\n",
     "\n",
     "Now let's create another `TextVectorization` layer and let's adapt it to the full IMDB training set (if the training set did not fit in RAM, we could just use a smaller sample of the training set by calling `train_set.take(500)`):"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 146,
    "metadata": {},
    "outputs": [],
    "source": [
     "max_vocabulary_size = 1000\n",
     "n_oov_buckets = 100\n",
     "\n",
     "sample_review_batches = train_set.map(lambda review, label: review)\n",
     "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
     "                                axis=0)\n",
     "\n",
     "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
     "                                       input_shape=[])\n",
     "text_vectorization.adapt(sample_reviews)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's run it on the same `X_example`, just to make sure the word IDs are larger now, since the vocabulary is bigger:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 147,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
        "array([[  9,  14,   2,  64,  64,  12,   5, 257,   9,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
        "       [  9,  13, 269, 531, 335,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
       ]
      },
      "execution_count": 147,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "text_vectorization(X_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Good! Now let's take a look at the first 10 words in the vocabulary:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 148,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
       ]
      },
      "execution_count": 148,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "text_vectorization.vocab[:10]"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "These are the most common words in the reviews."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now to build our model we will need to encode all these word IDs somehow. One approach is to create bags of words: for each review, and for each word in the vocabulary, we count the number of occurences of that word in the review. For example:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 149,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
        "array([[2., 2., 0., 1.],\n",
        "       [3., 0., 2., 0.]], dtype=float32)>"
       ]
      },
      "execution_count": 149,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
     "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "The first review has 2 times the word 0, 2 times the word 1, 0 times the word 2, and 1 time the word 3, so its bag-of-words representation is `[2, 2, 0, 1]`. Similarly, the second review has 3 times the word 0, 0 times the word 1, and so on. Let's wrap this logic in a small custom layer, and let's test it. We'll drop the counts for the word 0, since this corresponds to the `<pad>` token, which we don't care about."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 150,
    "metadata": {},
    "outputs": [],
    "source": [
     "class BagOfWords(keras.layers.Layer):\n",
     "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
     "        super().__init__(dtype=dtype, **kwargs)\n",
     "        self.n_tokens = n_tokens\n",
     "    def call(self, inputs):\n",
     "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
     "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's test it:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 151,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
        "array([[2., 0., 1.],\n",
        "       [0., 2., 0.]], dtype=float32)>"
       ]
      },
      "execution_count": 151,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "bag_of_words = BagOfWords(n_tokens=4)\n",
     "bag_of_words(simple_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "It works fine! Now let's create another `BagOfWord` with the right vocabulary size for our training set:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 152,
    "metadata": {},
    "outputs": [],
    "source": [
     "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n",
     "bag_of_words = BagOfWords(n_tokens)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "We're ready to train the model!"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 153,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/5\n",
       "782/782 [==============================] - 5s 5ms/step - loss: 0.5834 - accuracy: 0.6784 - val_loss: 0.5116 - val_accuracy: 0.7376\n",
       "Epoch 2/5\n",
       "782/782 [==============================] - 5s 5ms/step - loss: 0.4647 - accuracy: 0.7738 - val_loss: 0.4998 - val_accuracy: 0.7445\n",
       "Epoch 3/5\n",
       "782/782 [==============================] - 5s 5ms/step - loss: 0.4141 - accuracy: 0.8062 - val_loss: 0.5025 - val_accuracy: 0.7457\n",
       "Epoch 4/5\n",
       "782/782 [==============================] - 5s 5ms/step - loss: 0.3506 - accuracy: 0.8536 - val_loss: 0.5308 - val_accuracy: 0.7465\n",
       "Epoch 5/5\n",
       "782/782 [==============================] - 5s 5ms/step - loss: 0.2642 - accuracy: 0.9039 - val_loss: 0.5681 - val_accuracy: 0.7351\n"
      ]
     },
     {
      "data": {
       "text/plain": [
        "<tensorflow.python.keras.callbacks.History at 0x7fd4f052da90>"
       ]
      },
      "execution_count": 153,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "model = keras.models.Sequential([\n",
     "    text_vectorization,\n",
     "    bag_of_words,\n",
     "    keras.layers.Dense(100, activation=\"relu\"),\n",
     "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
     "])\n",
     "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
     "              metrics=[\"accuracy\"])\n",
     "model.fit(train_set, epochs=5, validation_data=valid_set)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "We get about 73.5% accuracy on the validation set after just the first epoch, but after that the model makes no significant progress. We will do better in Chapter 16. For now the point is just to perform efficient preprocessing using `tf.data` and Keras preprocessing layers."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### e.\n",
     "_Exercise: Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model._"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "To compute the mean embedding for each review, and multiply it by the square root of the number of words in that review, we will need a little function. For each sentence, this function needs to compute $M \\times \\sqrt N$, where $M$ is the mean of all the word embeddings in the sentence (excluding padding tokens), and $N$ is the number of words in the sentence (also excluding padding tokens). We can rewrite $M$ as $\\dfrac{S}{N}$, where $S$ is the sum of all word embeddings (it does not matter whether or not we include the padding tokens in this sum, since their representation is a zero vector). So the function must return $M \\times \\sqrt N = \\dfrac{S}{N} \\times \\sqrt N = \\dfrac{S}{\\sqrt N \\times \\sqrt N} \\times \\sqrt N= \\dfrac{S}{\\sqrt N}$."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 154,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
        "array([[3.535534 , 4.9497476, 2.1213205],\n",
        "       [6.       , 0.       , 0.       ]], dtype=float32)>"
       ]
      },
      "execution_count": 154,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def compute_mean_embedding(inputs):\n",
     "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
     "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
     "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
     "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
     "\n",
     "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
     "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
     "compute_mean_embedding(another_example)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's check that this is correct. The first review contains 2 words (the last token is a zero vector, which represents the `<pad>` token). Let's compute the mean embedding for these 2 words, and multiply the result by the square root of 2:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 155,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3.535534 , 4.9497476, 2.1213202]], dtype=float32)>"
       ]
      },
      "execution_count": 155,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Looks good! Now let's check the second review, which contains just one word (we ignore the two padding tokens):"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 156,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[6., 0., 0.]], dtype=float32)>"
       ]
      },
      "execution_count": 156,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Perfect. Now we're ready to train our final model. It's the same as before, except we replaced the `BagOfWords` layer with an `Embedding` layer followed by a `Lambda` layer that calls the `compute_mean_embedding` layer:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 157,
    "metadata": {},
    "outputs": [],
    "source": [
     "embedding_size = 20\n",
     "\n",
     "model = keras.models.Sequential([\n",
     "    text_vectorization,\n",
     "    keras.layers.Embedding(input_dim=n_tokens,\n",
     "                           output_dim=embedding_size,\n",
     "                           mask_zero=True), # <pad> tokens => zero vectors\n",
     "    keras.layers.Lambda(compute_mean_embedding),\n",
     "    keras.layers.Dense(100, activation=\"relu\"),\n",
     "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
     "])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### f.\n",
     "_Exercise: Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible._"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 158,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/5\n",
       "782/782 [==============================] - 3s 2ms/step - loss: 0.6053 - accuracy: 0.6568 - val_loss: 0.5151 - val_accuracy: 0.7382\n",
       "Epoch 2/5\n",
       "782/782 [==============================] - 2s 2ms/step - loss: 0.4922 - accuracy: 0.7569 - val_loss: 0.5081 - val_accuracy: 0.7466\n",
       "Epoch 3/5\n",
       "782/782 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7628 - val_loss: 0.4978 - val_accuracy: 0.7473\n",
       "Epoch 4/5\n",
       "782/782 [==============================] - 2s 2ms/step - loss: 0.4761 - accuracy: 0.7656 - val_loss: 0.4959 - val_accuracy: 0.7513\n",
       "Epoch 5/5\n",
       "782/782 [==============================] - 3s 2ms/step - loss: 0.4737 - accuracy: 0.7687 - val_loss: 0.4978 - val_accuracy: 0.7471\n"
      ]
     },
     {
      "data": {
       "text/plain": [
        "<tensorflow.python.keras.callbacks.History at 0x7f89584c3690>"
       ]
      },
      "execution_count": 158,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
     "model.fit(train_set, epochs=5, validation_data=valid_set)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "The model is not better using embeddings (but we will do better in Chapter 16). The pipeline looks fast enough (we optimized it earlier)."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### g.\n",
     "_Exercise: Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`._"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 159,
    "metadata": {},
    "outputs": [],
    "source": [
     "import tensorflow_datasets as tfds\n",
     "\n",
     "datasets = tfds.load(name=\"imdb_reviews\")\n",
     "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 160,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
       "tf.Tensor(0, shape=(), dtype=int64)\n"
      ]
     }
    ],
    "source": [
     "for example in train_set.take(1):\n",
     "    print(example[\"text\"])\n",
     "    print(example[\"label\"])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": []
   }
  ]