[
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Using Clustering as Preprocessing for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "clf.score(X_valid_pca, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reduced = best_model.transform(X_train_pca)\n",
    "X_valid_reduced = best_model.transform(X_valid_pca)\n",
    "X_test_reduced = best_model.transform(X_test_pca)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "clf.fit(X_train_reduced, y_train)\n",
    "    \n",
    "clf.score(X_valid_reduced, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That's not better at all! Let's see if tuning the number of clusters helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a `GridSearchCV` like we did earlier in this notebook, but since we already have a validation set, we don't need K-fold cross-validation, and we're only exploring a single hyperparameter, so it's simpler to just run a loop manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.3875\n",
      "10 0.575\n",
      "15 0.6\n",
      "20 0.6625\n",
      "25 0.6625\n",
      "30 0.6625\n",
      "35 0.675\n",
      "40 0.75\n",
      "45 0.7375\n",
      "50 0.725\n",
      "55 0.7125\n",
      "60 0.7125\n",
      "65 0.7375\n",
      "70 0.7375\n",
      "75 0.7375\n",
      "80 0.7875\n",
      "85 0.7625\n",
      "90 0.75\n",
      "95 0.7125\n",
      "100 0.775\n",
      "105 0.75\n",
      "110 0.725\n",
      "115 0.7625\n",
      "120 0.7\n",
      "125 0.75\n",
      "130 0.725\n",
      "135 0.7375\n",
      "140 0.7625\n",
      "145 0.6875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for n_clusters in k_range:\n",
    "    pipeline = Pipeline([\n",
    "        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42)),\n",
    "        (\"forest_clf\", RandomForestClassifier(n_estimators=150, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train_pca, y_train)\n",
    "    print(n_clusters, pipeline.score(X_valid_pca, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well, even by tuning the number of clusters, we never get beyond 80% accuracy. Looks like the distances to the cluster centroids are not as informative as the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extended = np.c_[X_train_pca, X_train_reduced]\n",
    "X_valid_extended = np.c_[X_valid_pca, X_valid_reduced]\n",
    "X_test_extended = np.c_[X_test_pca, X_test_reduced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "clf.fit(X_train_extended, y_train)\n",
    "clf.score(X_valid_extended, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better, but still worse than without the cluster features. The clusters are not useful to directly train a classifier in this case (but they can still help when labelling new training instances)."
   ]
  }
]