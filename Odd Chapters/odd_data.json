[
	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The MNIST dataset",
		"question": "Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set.",
		"code": "sol3.1.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The MNIST dataset",
		"question": "Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel .5 Then, for each image in the training set, create four shifted copies(one per direction) and add them to the training set.Finally, train your best model on this expanded training set and measure its accuracy on the test set.",
		"code": "sol3.2.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["os", "os"],
			["urllib.request", "urllib.request"]
		],
		"dataset description": "A dataset of passengers on the Titanic, found at https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/.",
		"question": "Tackle the Titanic dataset. predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.",
		"code": "sol3.3.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The iris dataset",
		"question": "Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.",
		"code": "sol5.8.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The MNIST dataset",
		"question": "Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one - versus - all to classify all 10 digits.You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach ?",
		"code": "sol5.9.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The California Housing dataset in scikit",
		"question": "Train an SVM regressor on the California housing dataset.",
		"code": "sol5.10.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The MNIST dataset in scikit",
		"question": "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set(e.g., use 50, 000 instances for training, 10,000 for validation, and 10, 000 for testing).Then train various classifiers, such as a Random Forest classifier, an Extra - Trees classifier, and an SVM.Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier.Once you have found one, try it on the test set.How much better does it perform compared to the individual classifiers ? ",
		"code": "sol7.8.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The MNIST dataset in scikit",
		"question": "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’ s class.Train a classifier on this new training set.Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble!Now let’ s evaluate the ensemble on the test set.For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’ s pre‐ dictions.How does it compare to the voting classifier you trained earlier ? ",
		"code": "sol7.9.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The Olivetti faces dataset in scikit,",
		"question": "The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the sklearn.datasets.fetch_olivetti_faces() function.",
		"code": "sol9.10.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The Olivetti faces dataset in scikit.",
		"question": "Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set",
		"code": "sol9.11.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"]
		],
		"dataset description": "The Olivetti faces dataset in scikit.",
		"question": "Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset's dimensionality (e.g., use PCA, preserving 99% of the variance).",
		"code": "sol9.12.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"]
		],
		"dataset description": "The Olivetti faces dataset in scikit.",
		"question": "a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function. b.Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset.You can load it with keras.datasets.cifar10.load_data().The dataset is composed of 60,000 32× 32– pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you 'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model' s architecture or hyperparameters. c.Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before ? Does it produce a better model ? How does it affect training speed ? d.Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self - normalizes(i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). e.Try regularizing the model with alpha dropout.Then, without retraining your model, see if you can achieve better accuracy using MC Dropout. f.Retrain your model using 1 cycle scheduling and see if it improves training speed and model accuracy. ",
		"code": "sol11.8.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"]
		],
		"dataset description": "The fashion MNIST dataset in scikit.",
		"question": "Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Note: for large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would lose a bit of image quality",
		"code": "so13.9.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"]
		],
		"dataset description": "The Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. Found at http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
		"question": "a. Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise. b.Split the test set into a validation set(15, 000) and a test set(10, 000). c.Use tf.data to create an efficient dataset for each set. d.Create a binary classification model, using a TextVectorization layer to preprocess each review.If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces.You should use a lookup table to output word indices, which must be prepared in the adapt() method. e.Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words(see Chapter 16).This rescaled mean embedding can then be passed to the rest of your model. f.Train the model and see what accuracy you get.Try to optimize your pipelines to make training as fast as possible. g.Use TFDS to load the same dataset more easily: tfds.load('imdb_reviews') ",
		"code": "sol13.10.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"]
		],
		"dataset description": "SketchRNN dataset",
		"question": "Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets",
		"code": "sol15.9.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"],
			["pandas", "pd"]
		],
		"dataset description": "The Bach chorales dataset. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. Found at https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/jsb_chorales.tgz",
		"question": "Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google's Coconet model, which was used for a nice Google doodle about Bach.",
		"code": "sol15.10.json"
	},

	{
		"packages": [
			["numpy", "np"],
			["scikit-learn", "sklearn"],
			["matplotlib", "mpl"],
			["tensorflow", "tf"],
			["pandas", "pd"]
		],
		"dataset description": "MNIST or CIFAR10 datasets",
		"question": "Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as CIFAR10 if you want a bigger challenge. Regardless of the dataset you're using, follow these steps: Split the dataset into a training set and a test set.Train a deep denoising autoencoder on the full training set. Check that the images are fairly well reconstructed.Visualize the images that most activate each neuron in the coding layer. Build a classification DNN, reusing the lower layers of the autoencoder.Train it using only 500 images from the training set.Does it perform better with or without pretraining ? ",
		"code": "sol17.9.json"
	}
]