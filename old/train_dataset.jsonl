{"prompt": "Code the answer to the following question in Python.\n\nDataset: Housing data https://raw.githubusercontent.com/ageron/handson-ml2/master/\n\nQuestion: Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don\u2019t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?\n\nCode:", "completion": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_prepared, housing_labels)\n\nnegative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\ngrid_search.best_params_\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n# see https://docs.scipy.org/doc/scipy/reference/stats.html\n# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n\n# Note: gamma is ignored when kernel is \"linear\"\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 200000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\nrnd_search.best_params_\n\nexpon_distrib = expon(scale=1.)\nsamples = expon_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Exponential distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\nreciprocal_distrib = reciprocal(20, 200000)\nsamples = reciprocal_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Reciprocal distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]\n\nk = 5\n\ntop_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices\n\nnp.array(attributes)[top_k_feature_indices]\n\nsorted(zip(feature_importances, attributes), reverse=True)[:k]\n\npreparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n\nhousing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)\n\nhousing_prepared_top_k_features[0:3]\n\nhousing_prepared[0:3, top_k_feature_indices]\n\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n\nprepare_select_and_predict_pipeline.fit(housing, housing_labels)\n\nsome_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\n\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n\nfull_pipeline.named_transformers_[\"cat\"].handle_unknown = 'ignore'\n\nparam_grid = [{\n    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n}]\n\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)\n\ngrid_search_prep.best_params_"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: the usual Iris dataset\n\nQuestion: Implement Batch Gradient Descent with early stopping for Softmax Regression\n(without using Scikit-Learn).\n\nCode:", "completion": "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nX_with_bias = np.c_[np.ones([len(X), 1]), X]\n\nnp.random.seed(2042)\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\ndef to_one_hot(y):\n    n_classes = y.max() + 1\n    m = len(y)\n    Y_one_hot = np.zeros((m, n_classes))\n    Y_one_hot[np.arange(m), y] = 1\n    return Y_one_hot\n\ny_train[:10]\n\nto_one_hot(y_train[:10])\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = np.sum(exps, axis=1, keepdims=True)\n    return exps / exp_sums\n\nn_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)\n\neta = 0.01\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error)\n    Theta = Theta - eta * gradients\n\nTheta\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\neta = 0.1\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # regularization hyperparameter\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n        loss = xentropy_loss + alpha * l2_loss\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\neta = 0.1 \nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # regularization hyperparameter\nbest_loss = np.infty\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n    logits = X_valid.dot(Theta)\n    Y_proba = softmax(logits)\n    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n    loss = xentropy_loss + alpha * l2_loss\n    if iteration % 500 == 0:\n        print(iteration, loss)\n    if loss < best_loss:\n        best_loss = loss\n    else:\n        print(iteration - 1, best_loss)\n        print(iteration, loss, \"early stopping!\")\n        break\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\nx0, x1 = np.meshgrid(\n        np.linspace(0, 8, 500).reshape(-1, 1),\n        np.linspace(0, 3.5, 200).reshape(-1, 1),\n    )\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n\nlogits = X_new_with_bias.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1, fontsize=12)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 7, 0, 3.5])\nplt.show()\n\nlogits = X_test.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_test)\naccuracy_score"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: moons dataset\n\nQuestion: Grow a forest.\na. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn\u2019s ShuffleSplit class for this.\nb. Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.\nc. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy\u2019s mode() function for this). This gives you majority-vote predictions over the test set.\nd. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!\n\nCode:", "completion": "from sklearn.model_selection import ShuffleSplit\n\nn_trees = 1000\nn_instances = 100\n\nmini_sets = []\n\nrs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\nfor mini_train_index, mini_test_index in rs.split(X_train):\n    X_mini_train = X_train[mini_train_index]\n    y_mini_train = y_train[mini_train_index]\n    mini_sets.append((X_mini_train, y_mini_train))\n\nfrom sklearn.base import clone\n\nforest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]\n\naccuracy_scores = []\n\nfor tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n    tree.fit(X_mini_train, y_mini_train)\n    \n    y_pred = tree.predict(X_test)\n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n\nnp.mean(accuracy_scores)\n\nY_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n\nfor tree_index, tree in enumerate(forest):\n    Y_pred[tree_index] = tree.predict(X_test)\n\nfrom scipy.stats import mode\n\ny_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n\naccuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: MNIST\n\nQuestion: Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image\u2019s target class. Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\n\nCode:", "completion": "np.random.seed(42)\n\nm = 10000\nidx = np.random.permutation(60000)[:m]\n\nX = mnist['data'][idx]\ny = mnist['target'][idx]\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)\n\nplt.figure(figsize=(13,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()\n\nplt.figure(figsize=(9,9))\ncmap = mpl.cm.get_cmap(\"jet\")\nfor digit in (2, 3, 5):\n    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\nidx = (y == 2) | (y == 3) | (y == 5) \nX_subset = X[idx]\ny_subset = y[idx]\n\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)\n\nplt.figure(figsize=(9,9))\nfor digit in (2, 3, 5):\n    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\ndef plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n    # Let's scale the input features so that they range from 0 to 1\n    X_normalized = MinMaxScaler().fit_transform(X)\n    # Now we create the list of coordinates of the digits plotted so far.\n    # We pretend that one is already plotted far away at the start, to\n    # avoid `if` statements in the loop below\n    neighbors = np.array([[10., 10.]])\n    # The rest should be self-explanatory\n    plt.figure(figsize=figsize)\n    cmap = mpl.cm.get_cmap(\"jet\")\n    digits = np.unique(y)\n    for digit in digits:\n        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])\n    plt.axis(\"off\")\n    ax = plt.gcf().gca()  # get current axes in current figure\n    for index, image_coord in enumerate(X_normalized):\n        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n        if closest_distance > min_distance:\n            neighbors = np.r_[neighbors, [image_coord]]\n            if images is None:\n                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n                         color=cmap(y[index] / 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n            else:\n                image = images[index].reshape(28, 28)\n                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                ax.add_artist(imagebox)\n\nplot_digits(X_reduced, y)\n\nplot_digits(X_reduced, y, images=X, figsize=(35, 25))\n\nplot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))\n\nfrom sklearn.decomposition import PCA\nimport time\n\nt0 = time.time()\nX_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"PCA took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_reduced, y)\nplt.show()\n\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nt0 = time.time()\nX_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"LLE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lle_reduced, y)\nplt.show()\n\nfrom sklearn.pipeline import Pipeline\n\npca_lle = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_lle_reduced = pca_lle.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+LLE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_lle_reduced, y)\nplt.show()\n\nfrom sklearn.manifold import MDS\n\nm = 2000\nt0 = time.time()\nX_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])\nt1 = time.time()\nprint(\"MDS took {:.1f}s (on just 2,000 MNIST images instead of 10,000).\".format(t1 - t0))\nplot_digits(X_mds_reduced, y[:m])\nplt.show()\n\nfrom sklearn.pipeline import Pipeline\n\npca_mds = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"mds\", MDS(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_mds_reduced = pca_mds.fit_transform(X[:2000])\nt1 = time.time()\nprint(\"PCA+MDS took {:.1f}s (on 2,000 MNIST images).\".format(t1 - t0))\nplot_digits(X_pca_mds_reduced, y[:2000])\nplt.show()\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nt0 = time.time()\nX_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nt1 = time.time()\nprint(\"LDA took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lda_reduced, y, figsize=(12,12))\nplt.show()\n\nfrom sklearn.manifold import TSNE\n\nt0 = time.time()\nX_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"t-SNE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_tsne_reduced, y)\nplt.show()\n\npca_tsne = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"tsne\", TSNE(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_tsne_reduced = pca_tsne.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+t-SNE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_tsne_reduced, y)\nplt.show()"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: MNIST\n\nQuestion: Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, use early stopping, plot learning curves using TensorBoard, and so on).\n\nCode:", "completion": "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n\nX_train_full.shape\n\nX_train_full.dtype\n\nX_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test / 255.\n\nplt.imshow(X_train[0], cmap=\"binary\")\nplt.axis('off')\nplt.show()\n\ny_train\n\nX_valid.shape\n\nX_test.shape\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(y_train[index], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n\nK = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nexpon_lr = ExponentialLearningRate(factor=1.005)\n\nhistory = model.fit(X_train, y_train, epochs=1,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[expon_lr])\n\nplt.plot(expon_lr.rates, expon_lr.losses)\nplt.gca().set_xscale('log')\nplt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\nplt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\nplt.grid()\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n              metrics=[\"accuracy\"])\n\nrun_index = 1 # increment this at every run\nrun_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\nrun_logdir\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n\nhistory = model.fit(X_train, y_train, epochs=100,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])\n\nmodel = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\nmodel.evaluate(X_test, y_test)"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: california_housing  from sklearn.datasets import fetch_california_housing\n\nQuestion: Implement a custom layer that performs Layer Normalization:\na. The build() method should define two trainable weights \u03b1 and \u03b2, both of shape input_shape[-1:] and data type tf.float32. \u03b1 should be initialized with 1s, and \u03b2 with 0s.\nb. The call() method should compute the mean \u03bc and standard deviation \u03c3 of each instance's features. For this, you can use tf.nn.moments(inputs, axes=-1, keepdims=True), which returns the mean \u03bc and the variance \u03c32 of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return \u03b1\u2297(X - \u03bc)/(\u03c3 + \u03b5) + \u03b2, where \u2297 represents itemwise multiplication (*) and \u03b5 is a smoothing term (small constant to avoid division by zero, e.g., 0.001).\nc. Ensure that your custom layer produces the same (or very nearly the same) output as the keras.layers.LayerNormalization layer.\n\nCode:", "completion": "class LayerNormalization(keras.layers.Layer):\n    def __init__(self, eps=0.001, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n\n    def build(self, batch_input_shape):\n        self.alpha = self.add_weight(\n            name=\"alpha\", shape=batch_input_shape[-1:],\n            initializer=\"ones\")\n        self.beta = self.add_weight(\n            name=\"beta\", shape=batch_input_shape[-1:],\n            initializer=\"zeros\")\n        super().build(batch_input_shape) # must be at the end\n\n    def call(self, X):\n        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"eps\": self.eps}\n\nX = X_train.astype(np.float32)\n\ncustom_layer_norm = LayerNormalization()\nkeras_layer_norm = keras.layers.LayerNormalization()\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X)))\n\nrandom_alpha = np.random.rand(X.shape[-1])\nrandom_beta = np.random.rand(X.shape[-1])\n\ncustom_layer_norm.set_weights([random_alpha, random_beta])\nkeras_layer_norm.set_weights([random_alpha, random_beta])\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X)))"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: Fashion MNIST\n\nQuestion: Train a model using a custom training loop to tackle the Fashion MNIST dataset.\na. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\nb. Try using a different optimizer with a different learning rate for the upper layers and the lower layers.\n\nCode:", "completion": "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test.astype(np.float32) / 255.\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape() as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                gradients = tape.gradient(loss, model.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states()\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nlower_layers = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n])\nupper_layers = keras.models.Sequential([\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\nmodel = keras.models.Sequential([\n    lower_layers, upper_layers\n])\n\nlower_optimizer = keras.optimizers.SGD(learning_rate=1e-4)\nupper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape(persistent=True) as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                for layers, optimizer in ((lower_layers, lower_optimizer),\n                                          (upper_layers, upper_optimizer)):\n                    gradients = tape.gradient(loss, layers.trainable_variables)\n                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n                del tape\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states()"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: MNIST\n\nQuestion: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n\nCode:", "completion": "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_full = X_train_full / 255.\nX_test = X_test / 255.\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\nX_train = X_train[..., np.newaxis]\nX_valid = X_valid[..., np.newaxis]\nX_test = X_test[..., np.newaxis]\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\nmodel.evaluate(X_test, y_test)"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: generated dataset\n\nQuestion: Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't.\n\nCode:", "completion": "default_reber_grammar = [\n    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n    [(\"T\", 3), (\"V\", 5)], # and so on...\n    [(\"X\", 3), (\"S\", 6)],\n    [(\"P\", 4), (\"V\", 6)],\n    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n\nembedded_reber_grammar = [\n    [(\"B\", 1)],\n    [(\"T\", 2), (\"P\", 3)],\n    [(default_reber_grammar, 4)],\n    [(default_reber_grammar, 5)],\n    [(\"T\", 6)],\n    [(\"P\", 6)],\n    [(\"E\", None)]]\n\ndef generate_string(grammar):\n    state = 0\n    output = []\n    while state is not None:\n        index = np.random.randint(len(grammar[state]))\n        production, state = grammar[state][index]\n        if isinstance(production, list):\n            production = generate_string(grammar=production)\n        output.append(production)\n    return \"\".join(output)\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(default_reber_grammar), end=\" \")\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(embedded_reber_grammar), end=\" \")\n\nPOSSIBLE_CHARS = \"BEPSTVX\"\n\ndef generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n    good_string = generate_string(grammar)\n    index = np.random.randint(len(good_string))\n    good_char = good_string[index]\n    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n    return good_string[:index] + bad_char + good_string[index + 1:]\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")\n\ndef string_to_ids(s, chars=POSSIBLE_CHARS):\n    return [chars.index(c) for c in s]\n\nstring_to_ids(\"BTTTXXVVETE\")\n\ndef generate_dataset(size):\n    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n                    for _ in range(size // 2)]\n    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n                   for _ in range(size - size // 2)]\n    all_strings = good_strings + bad_strings\n    X = tf.ragged.constant(all_strings, ragged_rank=1)\n    y = np.array([[1.] for _ in range(len(good_strings))] +\n                 [[0.] for _ in range(len(bad_strings))])\n    return X, y\n\nnp.random.seed(42)\n\nX_train, y_train = generate_dataset(10000)\nX_valid, y_valid = generate_dataset(2000)\n\nX_train[0]\n\ny_train[0]\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nembedding_size = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n    keras.layers.GRU(30),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n\ntest_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\nX_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n\ny_proba = model.predict(X_test)\nprint()\nprint(\"Estimated probability that these are Reber strings:\")\nfor index, string in enumerate(test_strings):\n    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: openai-gpt\n\nQuestion: Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text.\n\nCode:", "completion": "from transformers import TFOpenAIGPTLMHeadModel\n\nmodel = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\nfrom transformers import OpenAIGPTTokenizer\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n\nprompt_text = \"This royal throne of kings, this sceptred isle\"\nencoded_prompt = tokenizer.encode(prompt_text,\n                                  add_special_tokens=False,\n                                  return_tensors=\"tf\")\nencoded_prompt\n\nnum_sequences = 5\nlength = 40\n\ngenerated_sequences = model.generate(\n    input_ids=encoded_prompt,\n    do_sample=True,\n    max_length=length + len(encoded_prompt[0]),\n    temperature=1.0,\n    top_k=0,\n    top_p=0.9,\n    repetition_penalty=1.0,\n    num_return_sequences=num_sequences,\n)\n\ngenerated_sequences\n\nfor sequence in generated_sequences:\n    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n    print(text)\n    print(\"-\" * 80)"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: OpenAI LunarLander-v2 environment\n\nQuestion: Use policy gradients to solve OpenAI Gym's LunarLander-v2 environment. You will need to install the Box2D dependencies (%pip install -U gym[box2d]).\n\nCode:", "completion": "env = gym.make(\"LunarLander-v2\")\n\nenv.observation_space\n\nenv.seed(42)\nobs = env.reset()\nobs\n\nenv.action_space\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_inputs = env.observation_space.shape[0]\nn_outputs = env.action_space.n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n    keras.layers.Dense(32, activation=\"relu\"),\n    keras.layers.Dense(n_outputs, activation=\"softmax\"),\n])\n\ndef lander_play_one_step(env, obs, model, loss_fn):\n    with tf.GradientTape() as tape:\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        loss = tf.reduce_mean(loss_fn(action, probas))\n    grads = tape.gradient(loss, model.trainable_variables)\n    obs, reward, done, info = env.step(action[0, 0].numpy())\n    return obs, reward, done, grads\n\ndef lander_play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n    all_rewards = []\n    all_grads = []\n    for episode in range(n_episodes):\n        current_rewards = []\n        current_grads = []\n        obs = env.reset()\n        for step in range(n_max_steps):\n            obs, reward, done, grads = lander_play_one_step(env, obs, model, loss_fn)\n            current_rewards.append(reward)\n            current_grads.append(grads)\n            if done:\n                break\n        all_rewards.append(current_rewards)\n        all_grads.append(current_grads)\n    return all_rewards, all_grads\n\ndef discount_rewards(rewards, discount_rate):\n    discounted = np.array(rewards)\n    for step in range(len(rewards) - 2, -1, -1):\n        discounted[step] += discounted[step + 1] * discount_rate\n    return discounted\n\ndef discount_and_normalize_rewards(all_rewards, discount_rate):\n    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n                              for rewards in all_rewards]\n    flat_rewards = np.concatenate(all_discounted_rewards)\n    reward_mean = flat_rewards.mean()\n    reward_std = flat_rewards.std()\n    return [(discounted_rewards - reward_mean) / reward_std\n            for discounted_rewards in all_discounted_rewards]\n\nn_iterations = 200\nn_episodes_per_update = 16\nn_max_steps = 1000\ndiscount_rate = 0.99\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.005)\nloss_fn = keras.losses.sparse_categorical_crossentropy\n\nenv.seed(42)\n\nmean_rewards = []\n\nfor iteration in range(n_iterations):\n    all_rewards, all_grads = lander_play_multiple_episodes(\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n    print(\"\\rIteration: {}/{}, mean reward: {:.1f}  \".format(\n        iteration + 1, n_iterations, mean_reward), end=\"\")\n    mean_rewards.append(mean_reward)\n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n                                                       discount_rate)\n    all_mean_grads = []\n    for var_index in range(len(model.trainable_variables)):\n        mean_grads = tf.reduce_mean(\n            [final_reward * all_grads[episode_index][step][var_index]\n             for episode_index, final_rewards in enumerate(all_final_rewards)\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\n        all_mean_grads.append(mean_grads)\n    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n\nimport matplotlib.pyplot as plt\n\nplt.plot(mean_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean reward\")\nplt.grid()\nplt.show()\n\ndef lander_render_policy_net(model, n_max_steps=500, seed=42):\n    frames = []\n    env = gym.make(\"LunarLander-v2\")\n    env.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    obs = env.reset()\n    for step in range(n_max_steps):\n        frames.append(env.render(mode=\"rgb_array\"))\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        obs, reward, done, info = env.step(action[0, 0].numpy())\n        if done:\n            break\n    env.close()\n    return frames\n\nframes = lander_render_policy_net(model, seed=42)\nplot_animation(frames)"}
{"prompt": "Code the answer to the following question in Python.\n\nDataset: TF-Agents\n\nQuestion: Use TF-Agents to train an agent that can achieve a superhuman level at SpaceInvaders-v4 using any of the available algorithms.\n\nCode:", "completion": "class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n    def skip_frames(self, num_skip):\n        for _ in range(num_skip):\n          super().step(0) # NOOP for num_skip steps\n    def reset(self, **kwargs):\n        obs = super().reset(**kwargs)\n        self.skip_frames(40)\n        return obs\n    def step(self, action):\n        lives_before_action = self.ale.lives()\n        obs, rewards, done, info = super().step(action)\n        if self.ale.lives() < lives_before_action and not done:\n            self.skip_frames(40)\n        return obs, rewards, done, info"}