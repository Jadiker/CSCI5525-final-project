
[  
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 91,
    "metadata": {},
    "outputs": [],
    "source": [
     "from datetime import date\n",
     "\n",
     "# cannot use strftime()'s %B format since it depends on the locale\n",
     "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
     "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
     "\n",
     "def random_dates(n_dates):\n",
     "    min_date = date(1000, 1, 1).toordinal()\n",
     "    max_date = date(9999, 12, 31).toordinal()\n",
     "\n",
     "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
     "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
     "\n",
     "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
     "    y = [dt.isoformat() for dt in dates]\n",
     "    return x, y"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Here are a few random dates, displayed in both the input format and the target format:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 92,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Input                    Target                   \n",
       "--------------------------------------------------\n",
       "September 20, 7075       7075-09-20               \n",
       "May 15, 8579             8579-05-15               \n",
       "January 11, 7103         7103-01-11               \n"
      ]
     }
    ],
    "source": [
     "np.random.seed(42)\n",
     "\n",
     "n_dates = 3\n",
     "x_example, y_example = random_dates(n_dates)\n",
     "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
     "print(\"-\" * 50)\n",
     "for idx in range(n_dates):\n",
     "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's get the list of all possible characters in the inputs:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 93,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
       ]
      },
      "execution_count": 93,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
     "INPUT_CHARS"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "And here's the list of possible characters in the outputs:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 94,
    "metadata": {},
    "outputs": [],
    "source": [
     "OUTPUT_CHARS = \"0123456789-\""
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 95,
    "metadata": {},
    "outputs": [],
    "source": [
     "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
     "    return [chars.index(c) for c in date_str]"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 96,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "[7, 11, 19, 22, 11, 16, 9, 11, 20, 38, 28, 26, 37, 38, 33, 26, 33, 31]"
       ]
      },
      "execution_count": 96,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "date_str_to_ids(x_example[0], INPUT_CHARS)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 97,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
       ]
      },
      "execution_count": 97,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 98,
    "metadata": {},
    "outputs": [],
    "source": [
     "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
     "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
     "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
     "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
     "\n",
     "def create_dataset(n_dates):\n",
     "    x, y = random_dates(n_dates)\n",
     "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 99,
    "metadata": {},
    "outputs": [],
    "source": [
     "np.random.seed(42)\n",
     "\n",
     "X_train, Y_train = create_dataset(10000)\n",
     "X_valid, Y_valid = create_dataset(2000)\n",
     "X_test, Y_test = create_dataset(2000)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 100,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
       ]
      },
      "execution_count": 100,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "Y_train[0]"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### First version: a very basic seq2seq model"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
     "\n",
     "Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 101,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/20\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 1.8111 - accuracy: 0.3533 - val_loss: 1.3581 - val_accuracy: 0.4965\n",
       "Epoch 2/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 1.3518 - accuracy: 0.5103 - val_loss: 1.1915 - val_accuracy: 0.5694\n",
       "Epoch 3/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 1.1706 - accuracy: 0.5908 - val_loss: 0.9983 - val_accuracy: 0.6398\n",
       "Epoch 4/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.9158 - accuracy: 0.6686 - val_loss: 0.8012 - val_accuracy: 0.6987\n",
       "Epoch 5/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.7058 - accuracy: 0.7308 - val_loss: 0.6224 - val_accuracy: 0.7599\n",
       "Epoch 6/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.7756 - accuracy: 0.7203 - val_loss: 0.6541 - val_accuracy: 0.7599\n",
       "Epoch 7/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.5379 - accuracy: 0.8034 - val_loss: 0.4174 - val_accuracy: 0.8440\n",
       "Epoch 8/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.4867 - accuracy: 0.8262 - val_loss: 0.4188 - val_accuracy: 0.8480\n",
       "Epoch 9/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.2979 - accuracy: 0.8951 - val_loss: 0.2549 - val_accuracy: 0.9126\n",
       "Epoch 10/20\n",
       "313/313 [==============================] - 5s 14ms/step - loss: 0.1785 - accuracy: 0.9479 - val_loss: 0.1461 - val_accuracy: 0.9594\n",
       "Epoch 11/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.1830 - accuracy: 0.9557 - val_loss: 0.1644 - val_accuracy: 0.9550\n",
       "Epoch 12/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0775 - accuracy: 0.9857 - val_loss: 0.0595 - val_accuracy: 0.9901\n",
       "Epoch 13/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0400 - accuracy: 0.9953 - val_loss: 0.0342 - val_accuracy: 0.9957\n",
       "Epoch 14/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0248 - accuracy: 0.9979 - val_loss: 0.0231 - val_accuracy: 0.9983\n",
       "Epoch 15/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0161 - accuracy: 0.9991 - val_loss: 0.0149 - val_accuracy: 0.9995\n",
       "Epoch 16/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0108 - accuracy: 0.9997 - val_loss: 0.0106 - val_accuracy: 0.9996\n",
       "Epoch 17/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0074 - accuracy: 0.9999 - val_loss: 0.0077 - val_accuracy: 0.9999\n",
       "Epoch 18/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9999\n",
       "Epoch 19/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
       "Epoch 20/20\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n"
      ]
     }
    ],
    "source": [
     "embedding_size = 32\n",
     "max_output_length = Y_train.shape[1]\n",
     "\n",
     "np.random.seed(42)\n",
     "tf.random.set_seed(42)\n",
     "\n",
     "encoder = keras.models.Sequential([\n",
     "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
     "                           output_dim=embedding_size,\n",
     "                           input_shape=[None]),\n",
     "    keras.layers.LSTM(128)\n",
     "])\n",
     "\n",
     "decoder = keras.models.Sequential([\n",
     "    keras.layers.LSTM(128, return_sequences=True),\n",
     "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
     "])\n",
     "\n",
     "model = keras.models.Sequential([\n",
     "    encoder,\n",
     "    keras.layers.RepeatVector(max_output_length),\n",
     "    decoder\n",
     "])\n",
     "\n",
     "optimizer = keras.optimizers.Nadam()\n",
     "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
     "              metrics=[\"accuracy\"])\n",
     "history = model.fit(X_train, Y_train, epochs=20,\n",
     "                    validation_data=(X_valid, Y_valid))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 102,
    "metadata": {},
    "outputs": [],
    "source": [
     "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
     "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
     "            for sequence in ids]"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now we can use the model to convert some dates"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 103,
    "metadata": {},
    "outputs": [],
    "source": [
     "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 104,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "2009-09-17\n",
       "1789-07-14\n"
      ]
     }
    ],
    "source": [
     "#ids = model.predict_classes(X_new)\n",
     "ids = np.argmax(model.predict(X_new), axis=-1)\n",
     "for date_str in ids_to_date_strs(ids):\n",
     "    print(date_str)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Perfect! :)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 105,
    "metadata": {},
    "outputs": [],
    "source": [
     "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 106,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "2020-01-02\n",
       "1789-02-14\n"
      ]
     }
    ],
    "source": [
     "#ids = model.predict_classes(X_new)\n",
     "ids = np.argmax(model.predict(X_new), axis=-1)\n",
     "for date_str in ids_to_date_strs(ids):\n",
     "    print(date_str)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 107,
    "metadata": {},
    "outputs": [],
    "source": [
     "max_input_length = X_train.shape[1]\n",
     "\n",
     "def prepare_date_strs_padded(date_strs):\n",
     "    X = prepare_date_strs(date_strs)\n",
     "    if X.shape[1] < max_input_length:\n",
     "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
     "    return X\n",
     "\n",
     "def convert_date_strs(date_strs):\n",
     "    X = prepare_date_strs_padded(date_strs)\n",
     "    #ids = model.predict_classes(X)\n",
     "    ids = np.argmax(model.predict(X), axis=-1)\n",
     "    return ids_to_date_strs(ids)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 108,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['2020-05-02', '1789-07-14']"
       ]
      },
      "execution_count": 108,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Second version: feeding the shifted targets to the decoder (teacher forcing)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
     "\n",
     "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
     "\n",
     "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
     "\n",
     "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
     "\n",
     "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 109,
    "metadata": {},
    "outputs": [],
    "source": [
     "sos_id = len(OUTPUT_CHARS) + 1\n",
     "\n",
     "def shifted_output_sequences(Y):\n",
     "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
     "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
     "\n",
     "X_train_decoder = shifted_output_sequences(Y_train)\n",
     "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
     "X_test_decoder = shifted_output_sequences(Y_test)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's take a look at the decoder's training inputs:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 110,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
        "array([[12,  8,  1, ..., 10, 11,  3],\n",
        "       [12,  9,  6, ...,  6, 11,  2],\n",
        "       [12,  8,  2, ...,  2, 11,  2],\n",
        "       ...,\n",
        "       [12, 10,  8, ...,  2, 11,  4],\n",
        "       [12,  2,  2, ...,  3, 11,  3],\n",
        "       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"
       ]
      },
      "execution_count": 110,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "X_train_decoder"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 111,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/10\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 1.6898 - accuracy: 0.3714 - val_loss: 1.4141 - val_accuracy: 0.4603\n",
       "Epoch 2/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 1.2118 - accuracy: 0.5541 - val_loss: 0.9360 - val_accuracy: 0.6653\n",
       "Epoch 3/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.6399 - accuracy: 0.7766 - val_loss: 0.4054 - val_accuracy: 0.8631\n",
       "Epoch 4/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.2207 - accuracy: 0.9463 - val_loss: 0.1069 - val_accuracy: 0.9869\n",
       "Epoch 5/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0805 - accuracy: 0.9910 - val_loss: 0.0445 - val_accuracy: 0.9976\n",
       "Epoch 6/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0297 - accuracy: 0.9993 - val_loss: 0.0237 - val_accuracy: 0.9992\n",
       "Epoch 7/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.0702 - val_accuracy: 0.9889\n",
       "Epoch 8/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0187 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9999\n",
       "Epoch 9/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
       "Epoch 10/10\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n"
      ]
     }
    ],
    "source": [
     "encoder_embedding_size = 32\n",
     "decoder_embedding_size = 32\n",
     "lstm_units = 128\n",
     "\n",
     "np.random.seed(42)\n",
     "tf.random.set_seed(42)\n",
     "\n",
     "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
     "encoder_embedding = keras.layers.Embedding(\n",
     "    input_dim=len(INPUT_CHARS) + 1,\n",
     "    output_dim=encoder_embedding_size)(encoder_input)\n",
     "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
     "    lstm_units, return_state=True)(encoder_embedding)\n",
     "encoder_state = [encoder_state_h, encoder_state_c]\n",
     "\n",
     "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
     "decoder_embedding = keras.layers.Embedding(\n",
     "    input_dim=len(OUTPUT_CHARS) + 2,\n",
     "    output_dim=decoder_embedding_size)(decoder_input)\n",
     "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
     "    decoder_embedding, initial_state=encoder_state)\n",
     "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
     "                                    activation=\"softmax\")(decoder_lstm_output)\n",
     "\n",
     "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
     "                           outputs=[decoder_output])\n",
     "\n",
     "optimizer = keras.optimizers.Nadam()\n",
     "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
     "              metrics=[\"accuracy\"])\n",
     "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
     "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "This model also reaches 100% validation accuracy, but it does so even faster."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 112,
    "metadata": {},
    "outputs": [],
    "source": [
     "sos_id = len(OUTPUT_CHARS) + 1\n",
     "\n",
     "def predict_date_strs(date_strs):\n",
     "    X = prepare_date_strs_padded(date_strs)\n",
     "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
     "    for index in range(max_output_length):\n",
     "        pad_size = max_output_length - Y_pred.shape[1]\n",
     "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
     "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
     "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
     "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
     "    return ids_to_date_strs(Y_pred[:, 1:])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 113,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['1789-07-14', '2020-05-01']"
       ]
      },
      "execution_count": 113,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Works fine! :)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Third version: using TF-Addons's seq2seq implementation"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's build exactly the same model, but using TF-Addon's seq2seq API. The implementation below is almost very similar to the TFA example higher in this notebook, except without the model input to specify the output sequence length, for simplicity (but you can easily add it back in if you need it for your projects, when the output sequences have very different lengths)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 114,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/15\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 1.6757 - accuracy: 0.3683 - val_loss: 1.4602 - val_accuracy: 0.4214\n",
       "Epoch 2/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 1.3873 - accuracy: 0.4566 - val_loss: 1.2904 - val_accuracy: 0.4957\n",
       "Epoch 3/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 1.0471 - accuracy: 0.6109 - val_loss: 0.7737 - val_accuracy: 0.7276\n",
       "Epoch 4/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.5056 - accuracy: 0.8296 - val_loss: 0.2695 - val_accuracy: 0.9305\n",
       "Epoch 5/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.1677 - accuracy: 0.9657 - val_loss: 0.0870 - val_accuracy: 0.9912\n",
       "Epoch 6/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.1007 - accuracy: 0.9850 - val_loss: 0.0492 - val_accuracy: 0.9975\n",
       "Epoch 7/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0308 - accuracy: 0.9993 - val_loss: 0.0228 - val_accuracy: 0.9996\n",
       "Epoch 8/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0168 - accuracy: 0.9999 - val_loss: 0.0144 - val_accuracy: 0.9999\n",
       "Epoch 9/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9999\n",
       "Epoch 10/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9999\n",
       "Epoch 11/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9999\n",
       "Epoch 12/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
       "Epoch 13/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
       "Epoch 14/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
       "Epoch 15/15\n",
       "313/313 [==============================] - 5s 15ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n"
      ]
     }
    ],
    "source": [
     "import tensorflow_addons as tfa\n",
     "\n",
     "np.random.seed(42)\n",
     "tf.random.set_seed(42)\n",
     "\n",
     "encoder_embedding_size = 32\n",
     "decoder_embedding_size = 32\n",
     "units = 128\n",
     "\n",
     "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
     "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
     "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
     "\n",
     "encoder_embeddings = keras.layers.Embedding(\n",
     "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
     "\n",
     "decoder_embedding_layer = keras.layers.Embedding(\n",
     "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
     "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
     "\n",
     "encoder = keras.layers.LSTM(units, return_state=True)\n",
     "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
     "encoder_state = [state_h, state_c]\n",
     "\n",
     "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
     "\n",
     "decoder_cell = keras.layers.LSTMCell(units)\n",
     "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
     "\n",
     "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
     "                                                 sampler,\n",
     "                                                 output_layer=output_layer)\n",
     "final_outputs, final_state, final_sequence_lengths = decoder(\n",
     "    decoder_embeddings,\n",
     "    initial_state=encoder_state)\n",
     "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
     "\n",
     "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
     "                           outputs=[Y_proba])\n",
     "optimizer = keras.optimizers.Nadam()\n",
     "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
     "              metrics=[\"accuracy\"])\n",
     "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
     "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "And once again, 100% validation accuracy! To use the model, we can just reuse the `predict_date_strs()` function:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 115,
    "metadata": {
     "scrolled": true
    },
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['1789-07-14', '2020-05-01']"
       ]
      },
      "execution_count": 115,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "However, there's a much more efficient way to perform inference. Until now, during inference, we've run the model once for each new character. Instead, we can create a new decoder, based on the previously trained layers, but using a `GreedyEmbeddingSampler` instead of a `TrainingSampler`.\n",
     "\n",
     "At each time step, the `GreedyEmbeddingSampler` will compute the argmax of the decoder's outputs, and run the resulting token IDs through the decoder's embedding layer. Then it will feed the resulting embeddings to the decoder's LSTM cell at the next time step. This way, we only need to run the decoder once to get the full prediction."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 116,
    "metadata": {},
    "outputs": [],
    "source": [
     "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
     "    embedding_fn=decoder_embedding_layer)\n",
     "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
     "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
     "    maximum_iterations=max_output_length)\n",
     "batch_size = tf.shape(encoder_inputs)[:1]\n",
     "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
     "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
     "    start_tokens,\n",
     "    initial_state=encoder_state,\n",
     "    start_tokens=start_tokens,\n",
     "    end_token=0)\n",
     "\n",
     "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
     "                                     outputs=[final_outputs.sample_id])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "A few notes:\n",
     "* The `GreedyEmbeddingSampler` needs the `start_tokens` (a vector containing the start-of-sequence ID for each decoder sequence), and the `end_token` (the decoder will stop decoding a sequence once the model outputs this token).\n",
     "* We must set `maximum_iterations` when creating the `BasicDecoder`, or else it may run into an infinite loop (if the model never outputs the end token for at least one of the sequences). This would force you would to restart the Jupyter kernel.\n",
     "* The decoder inputs are not needed anymore, since all the decoder inputs are generated dynamically based on the outputs from the previous time step.\n",
     "* The model's outputs are `final_outputs.sample_id` instead of the softmax of `final_outputs.rnn_outputs`. This allows us to directly get the argmax of the model's outputs. If you prefer to have access to the logits, you can replace `final_outputs.sample_id` with `final_outputs.rnn_outputs`."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Now we can write a simple function that uses the model to perform the date format conversion:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 117,
    "metadata": {},
    "outputs": [],
    "source": [
     "def fast_predict_date_strs(date_strs):\n",
     "    X = prepare_date_strs_padded(date_strs)\n",
     "    Y_pred = inference_model.predict(X)\n",
     "    return ids_to_date_strs(Y_pred)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 118,
    "metadata": {
     "scrolled": true
    },
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['1789-07-14', '2020-05-01']"
       ]
      },
      "execution_count": 118,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's check that it really is faster:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 119,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "199 ms ± 3.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
      ]
     }
    ],
    "source": [
     "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 120,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "18.3 ms ± 366 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
      ]
     }
    ],
    "source": [
     "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "That's more than a 10x speedup! And it would be even more if we were handling longer sequences."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Fourth version: using TF-Addons's seq2seq implementation with a scheduled sampler"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "**Warning**: due to a TF bug, this version only works using TensorFlow 2.2 or above."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "When we trained the previous model, at each time step _t_ we gave the model the target token for time step _t_ - 1. However, at inference time, the model did not get the previous target at each time step. Instead, it got the previous prediction. So there is a discrepancy between training and inference, which may lead to disappointing performance. To alleviate this, we can gradually replace the targets with the predictions, during training. For this, we just need to replace the `TrainingSampler` with a `ScheduledEmbeddingTrainingSampler`, and use a Keras callback to gradually increase the `sampling_probability` (i.e., the probability that the decoder will use the prediction from the previous time step rather than the target for the previous time step)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 121,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/20\n"
      ]
     },
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
       "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
       "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "313/313 [==============================] - 6s 19ms/step - loss: 1.6759 - accuracy: 0.3681 - val_loss: 1.4611 - val_accuracy: 0.4198\n",
       "Epoch 2/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 1.3872 - accuracy: 0.4583 - val_loss: 1.2827 - val_accuracy: 0.5021\n",
       "Epoch 3/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 1.0425 - accuracy: 0.6152 - val_loss: 0.8165 - val_accuracy: 0.7000\n",
       "Epoch 4/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.6353 - accuracy: 0.7673 - val_loss: 0.4365 - val_accuracy: 0.8464\n",
       "Epoch 5/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.3764 - accuracy: 0.8765 - val_loss: 0.2795 - val_accuracy: 0.9166\n",
       "Epoch 6/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.2506 - accuracy: 0.9269 - val_loss: 0.1805 - val_accuracy: 0.9489\n",
       "Epoch 7/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.1427 - accuracy: 0.9625 - val_loss: 0.1115 - val_accuracy: 0.9718\n",
       "Epoch 8/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.0853 - accuracy: 0.9804 - val_loss: 0.0785 - val_accuracy: 0.9809\n",
       "Epoch 9/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.1010 - accuracy: 0.9797 - val_loss: 0.1198 - val_accuracy: 0.9746\n",
       "Epoch 10/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.0447 - accuracy: 0.9917 - val_loss: 0.0306 - val_accuracy: 0.9949\n",
       "Epoch 11/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0241 - accuracy: 0.9961 - val_loss: 0.0205 - val_accuracy: 0.9968\n",
       "Epoch 12/20\n",
       "313/313 [==============================] - 5s 17ms/step - loss: 0.0705 - accuracy: 0.9861 - val_loss: 0.0823 - val_accuracy: 0.9860\n",
       "Epoch 13/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0182 - accuracy: 0.9977 - val_loss: 0.0117 - val_accuracy: 0.9980\n",
       "Epoch 14/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.0085 - val_accuracy: 0.9990\n",
       "Epoch 15/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9993\n",
       "Epoch 16/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.0048 - val_accuracy: 0.9996\n",
       "Epoch 17/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.0039 - val_accuracy: 0.9995\n",
       "Epoch 18/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0024 - val_accuracy: 0.9999\n",
       "Epoch 19/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0031 - val_accuracy: 0.9992\n",
       "Epoch 20/20\n",
       "313/313 [==============================] - 5s 16ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0022 - val_accuracy: 0.9999\n"
      ]
     }
    ],
    "source": [
     "import tensorflow_addons as tfa\n",
     "\n",
     "np.random.seed(42)\n",
     "tf.random.set_seed(42)\n",
     "\n",
     "n_epochs = 20\n",
     "encoder_embedding_size = 32\n",
     "decoder_embedding_size = 32\n",
     "units = 128\n",
     "\n",
     "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
     "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
     "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
     "\n",
     "encoder_embeddings = keras.layers.Embedding(\n",
     "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
     "\n",
     "decoder_embedding_layer = keras.layers.Embedding(\n",
     "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
     "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
     "\n",
     "encoder = keras.layers.LSTM(units, return_state=True)\n",
     "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
     "encoder_state = [state_h, state_c]\n",
     "\n",
     "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n",
     "    sampling_probability=0.,\n",
     "    embedding_fn=decoder_embedding_layer)\n",
     "# we must set the sampling_probability after creating the sampler\n",
     "# (see https://github.com/tensorflow/addons/pull/1714)\n",
     "sampler.sampling_probability = tf.Variable(0.)\n",
     "\n",
     "decoder_cell = keras.layers.LSTMCell(units)\n",
     "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
     "\n",
     "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
     "                                                 sampler,\n",
     "                                                 output_layer=output_layer)\n",
     "final_outputs, final_state, final_sequence_lengths = decoder(\n",
     "    decoder_embeddings,\n",
     "    initial_state=encoder_state)\n",
     "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
     "\n",
     "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
     "                           outputs=[Y_proba])\n",
     "optimizer = keras.optimizers.Nadam()\n",
     "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
     "              metrics=[\"accuracy\"])\n",
     "\n",
     "def update_sampling_probability(epoch, logs):\n",
     "    proba = min(1.0, epoch / (n_epochs - 10))\n",
     "    sampler.sampling_probability.assign(proba)\n",
     "\n",
     "sampling_probability_cb = keras.callbacks.LambdaCallback(\n",
     "    on_epoch_begin=update_sampling_probability)\n",
     "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n",
     "                    validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
     "                    callbacks=[sampling_probability_cb])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Not quite 100% validation accuracy, but close enough!"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "For inference, we could do the exact same thing as earlier, using a `GreedyEmbeddingSampler`. However, just for the sake of completeness, let's use a `SampleEmbeddingSampler` instead. It's almost the same thing, except that instead of using the argmax of the model's output to find the token ID, it treats the outputs as logits and uses them to sample a token ID randomly. This can be useful when you want to generate text. The `softmax_temperature` argument serves the \n",
     "same purpose as when we generated Shakespeare-like text (the higher this argument, the more random the generated text will be)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 122,
    "metadata": {},
    "outputs": [],
    "source": [
     "softmax_temperature = tf.Variable(1.)\n",
     "\n",
     "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n",
     "    embedding_fn=decoder_embedding_layer,\n",
     "    softmax_temperature=softmax_temperature)\n",
     "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
     "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
     "    maximum_iterations=max_output_length)\n",
     "batch_size = tf.shape(encoder_inputs)[:1]\n",
     "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
     "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
     "    start_tokens,\n",
     "    initial_state=encoder_state,\n",
     "    start_tokens=start_tokens,\n",
     "    end_token=0)\n",
     "\n",
     "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
     "                                     outputs=[final_outputs.sample_id])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 123,
    "metadata": {},
    "outputs": [],
    "source": [
     "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
     "    softmax_temperature.assign(temperature)\n",
     "    X = prepare_date_strs_padded(date_strs)\n",
     "    Y_pred = inference_model.predict(X)\n",
     "    return ids_to_date_strs(Y_pred)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 124,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['1789-07-14', '2020-05-01']"
       ]
      },
      "execution_count": 124,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "tf.random.set_seed(42)\n",
     "\n",
     "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Dates look good at room temperature. Now let's heat things up a bit:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 125,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['2289607-12', '9272-03-01']"
       ]
      },
      "execution_count": 125,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "tf.random.set_seed(42)\n",
     "\n",
     "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"],\n",
     "                           temperature=5.)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Oops, the dates are overcooked, now. Let's call them \"creative\" dates."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Fifth version: using TFA seq2seq, the Keras subclassing API and attention mechanisms"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "The sequences in this problem are pretty short, but if we wanted to tackle longer sequences, we would probably have to use attention mechanisms. While it's possible to code our own implementation, it's simpler and more efficient to use TF-Addons's implementation instead. Let's do that now, this time using Keras' subclassing API.\n",
     "\n",
     "**Warning**: due to a TensorFlow bug (see [this issue](https://github.com/tensorflow/addons/issues/1153) for details), the `get_initial_state()` method fails in eager mode, so for now we have to use the subclassing API, as Keras automatically calls `tf.function()` on the `call()` method (so it runs in graph mode)."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "In this implementation, we've reverted back to using the `TrainingSampler`, for simplicity (but you can easily tweak it to use a `ScheduledEmbeddingTrainingSampler` instead). We also use a `GreedyEmbeddingSampler` during inference, so this class is pretty easy to use:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 126,
    "metadata": {},
    "outputs": [],
    "source": [
     "class DateTranslation(keras.models.Model):\n",
     "    def __init__(self, units=128, encoder_embedding_size=32,\n",
     "                 decoder_embedding_size=32, **kwargs):\n",
     "        super().__init__(**kwargs)\n",
     "        self.encoder_embedding = keras.layers.Embedding(\n",
     "            input_dim=len(INPUT_CHARS) + 1,\n",
     "            output_dim=encoder_embedding_size)\n",
     "        self.encoder = keras.layers.LSTM(units,\n",
     "                                         return_sequences=True,\n",
     "                                         return_state=True)\n",
     "        self.decoder_embedding = keras.layers.Embedding(\n",
     "            input_dim=len(OUTPUT_CHARS) + 2,\n",
     "            output_dim=decoder_embedding_size)\n",
     "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
     "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
     "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
     "            cell=decoder_inner_cell,\n",
     "            attention_mechanism=self.attention)\n",
     "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
     "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
     "            cell=self.decoder_cell,\n",
     "            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
     "            output_layer=output_layer)\n",
     "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
     "            cell=self.decoder_cell,\n",
     "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
     "                embedding_fn=self.decoder_embedding),\n",
     "            output_layer=output_layer,\n",
     "            maximum_iterations=max_output_length)\n",
     "\n",
     "    def call(self, inputs, training=None):\n",
     "        encoder_input, decoder_input = inputs\n",
     "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
     "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
     "            encoder_embeddings,\n",
     "            training=training)\n",
     "        encoder_state = [encoder_state_h, encoder_state_c]\n",
     "\n",
     "        self.attention(encoder_outputs,\n",
     "                       setup_memory=True)\n",
     "        \n",
     "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
     "\n",
     "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
     "            decoder_embeddings)\n",
     "        decoder_initial_state = decoder_initial_state.clone(\n",
     "            cell_state=encoder_state)\n",
     "        \n",
     "        if training:\n",
     "            decoder_outputs, _, _ = self.decoder(\n",
     "                decoder_embeddings,\n",
     "                initial_state=decoder_initial_state,\n",
     "                training=training)\n",
     "        else:\n",
     "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n",
     "            decoder_outputs, _, _ = self.inference_decoder(\n",
     "                decoder_embeddings,\n",
     "                initial_state=decoder_initial_state,\n",
     "                start_tokens=start_tokens,\n",
     "                end_token=0)\n",
     "\n",
     "        return tf.nn.softmax(decoder_outputs.rnn_output)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 127,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Epoch 1/25\n",
       "313/313 [==============================] - 7s 21ms/step - loss: 2.1549 - accuracy: 0.2295 - val_loss: 2.1450 - val_accuracy: 0.2239\n",
       "Epoch 2/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 1.8147 - accuracy: 0.3492 - val_loss: 1.4931 - val_accuracy: 0.4476\n",
       "Epoch 3/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 1.3585 - accuracy: 0.4909 - val_loss: 1.3168 - val_accuracy: 0.5100\n",
       "Epoch 4/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 1.2787 - accuracy: 0.5293 - val_loss: 1.1767 - val_accuracy: 0.5624\n",
       "Epoch 5/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 1.1236 - accuracy: 0.5776 - val_loss: 1.0769 - val_accuracy: 0.5907\n",
       "Epoch 6/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 1.0369 - accuracy: 0.6073 - val_loss: 1.0159 - val_accuracy: 0.6199\n",
       "Epoch 7/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.9752 - accuracy: 0.6295 - val_loss: 0.9723 - val_accuracy: 0.6346\n",
       "Epoch 8/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.9794 - accuracy: 0.6315 - val_loss: 0.9444 - val_accuracy: 0.6371\n",
       "Epoch 9/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.9338 - accuracy: 0.6415 - val_loss: 0.9296 - val_accuracy: 0.6381\n",
       "Epoch 10/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.9439 - accuracy: 0.6418 - val_loss: 0.9028 - val_accuracy: 0.6574\n",
       "Epoch 11/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.8807 - accuracy: 0.6637 - val_loss: 0.9835 - val_accuracy: 0.6369\n",
       "Epoch 12/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.7307 - accuracy: 0.6953 - val_loss: 0.8942 - val_accuracy: 0.6873\n",
       "Epoch 13/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.5833 - accuracy: 0.7327 - val_loss: 0.6944 - val_accuracy: 0.7391\n",
       "Epoch 14/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.4664 - accuracy: 0.7940 - val_loss: 0.6228 - val_accuracy: 0.7885\n",
       "Epoch 15/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.3205 - accuracy: 0.8740 - val_loss: 0.4825 - val_accuracy: 0.8780\n",
       "Epoch 16/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.2329 - accuracy: 0.9216 - val_loss: 0.3851 - val_accuracy: 0.9118\n",
       "Epoch 17/25\n",
       "313/313 [==============================] - 7s 21ms/step - loss: 0.2480 - accuracy: 0.9372 - val_loss: 0.2785 - val_accuracy: 0.9111\n",
       "Epoch 18/25\n",
       "313/313 [==============================] - 7s 22ms/step - loss: 0.1182 - accuracy: 0.9801 - val_loss: 0.1372 - val_accuracy: 0.9786\n",
       "Epoch 19/25\n",
       "313/313 [==============================] - 7s 22ms/step - loss: 0.0643 - accuracy: 0.9937 - val_loss: 0.0681 - val_accuracy: 0.9909\n",
       "Epoch 20/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.0446 - accuracy: 0.9952 - val_loss: 0.0487 - val_accuracy: 0.9934\n",
       "Epoch 21/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.0247 - accuracy: 0.9987 - val_loss: 0.0228 - val_accuracy: 0.9987\n",
       "Epoch 22/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.0456 - accuracy: 0.9918 - val_loss: 0.0207 - val_accuracy: 0.9985\n",
       "Epoch 23/25\n",
       "313/313 [==============================] - 6s 18ms/step - loss: 0.0131 - accuracy: 0.9997 - val_loss: 0.0127 - val_accuracy: 0.9993\n",
       "Epoch 24/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.0360 - accuracy: 0.9933 - val_loss: 0.0146 - val_accuracy: 0.9990\n",
       "Epoch 25/25\n",
       "313/313 [==============================] - 6s 19ms/step - loss: 0.0092 - accuracy: 0.9998 - val_loss: 0.0089 - val_accuracy: 0.9992\n"
      ]
     }
    ],
    "source": [
     "np.random.seed(42)\n",
     "tf.random.set_seed(42)\n",
     "\n",
     "model = DateTranslation()\n",
     "optimizer = keras.optimizers.Nadam()\n",
     "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
     "              metrics=[\"accuracy\"])\n",
     "history = model.fit([X_train, X_train_decoder], Y_train, epochs=25,\n",
     "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Not quite 100% validation accuracy, but close. It took a bit longer to converge this time, but there were also more parameters and more computations per iteration. And we did not use a scheduled sampler.\n",
     "\n",
     "To use the model, we can write yet another little function:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 128,
    "metadata": {},
    "outputs": [],
    "source": [
     "def fast_predict_date_strs_v2(date_strs):\n",
     "    X = prepare_date_strs_padded(date_strs)\n",
     "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
     "    Y_probas = model.predict([X, X_decoder])\n",
     "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
     "    return ids_to_date_strs(Y_pred)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 129,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['1789-07-14', '2020-05-01']"
       ]
      },
      "execution_count": 129,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "There are still a few interesting features from TF-Addons that you may want to look at:\n",
     "* Using a `BeamSearchDecoder` rather than a `BasicDecoder` for inference. Instead of outputing the character with the highest probability, this decoder keeps track of the several candidates, and keeps only the most likely sequences of candidates (see chapter 16 in the book for more details).\n",
     "* Setting masks or specifying `sequence_length` if the input or target sequences may have very different lengths.\n",
     "* Using a `ScheduledOutputTrainingSampler`, which gives you more flexibility than the `ScheduledEmbeddingTrainingSampler` to decide how to feed the output at time _t_ to the cell at time _t_+1. By default it feeds the outputs directly to cell, without computing the argmax ID and passing it through an embedding layer. Alternatively, you specify a `next_inputs_fn` function that will be used to convert the cell outputs to inputs at the next step."
    ]
   }
 ]
 