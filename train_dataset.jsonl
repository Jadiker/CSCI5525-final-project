{"prompt": "The MNIST dataset\n\nQuestion: Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set.\n\nCode:", "completion": " from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n\nknn_clf = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_\n\ngrid_search.best_score_\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = grid_search.predict(X_test)\naccuracy_score(y_test, y_pred) END"}
{"prompt": "A dataset of passengers on the Titanic, found at https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/.\n\nQuestion: Tackle the Titanic dataset. predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.\n\nCode:", "completion": " import os\nimport urllib.request\n\nTITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\nDOWNLOAD_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/\"\n\ndef fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    for filename in (\"train.csv\", \"test.csv\"):\n        filepath = os.path.join(path, filename)\n        if not os.path.isfile(filepath):\n            print(\"Downloading\", filename)\n            urllib.request.urlretrieve(url + filename, filepath)\n\nfetch_titanic_data()    \n\nimport pandas as pd\n\ndef load_titanic_data(filename, titanic_path=TITANIC_PATH):\n    csv_path = os.path.join(titanic_path, filename)\n    return pd.read_csv(csv_path)\n\ntrain_data = load_titanic_data(\"train.csv\")\ntest_data = load_titanic_data(\"test.csv\")\n\ntrain_data.head()\n\ntrain_data = train_data.set_index(\"PassengerId\")\ntest_data = test_data.set_index(\"PassengerId\")\n\ntrain_data.info()\n\ntrain_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n\ntrain_data.describe()\n\ntrain_data[\"Survived\"].value_counts()\n\ntrain_data[\"Pclass\"].value_counts()\n\ntrain_data[\"Sex\"].value_counts()\n\ntrain_data[\"Embarked\"].value_counts()\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler())\n    ])\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n\npreprocess_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])\n\nX_train = preprocess_pipeline.fit_transform(\n    train_data[num_attribs + cat_attribs])\nX_train\n\ny_train = train_data[\"Survived\"]\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\n\nX_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\ny_pred = forest_clf.predict(X_test)\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.show()\n\ntrain_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\ntrain_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n\ntrain_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntrain_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\nHAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\nSPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\nSPAM_PATH = os.path.join(\"datasets\", \"spam\")\n\ndef fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n    if not os.path.isdir(spam_path):\n        os.makedirs(spam_path)\n    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n        path = os.path.join(spam_path, filename)\n        if not os.path.isfile(path):\n            urllib.request.urlretrieve(url, path)\n        tar_bz2_file = tarfile.open(path)\n        tar_bz2_file.extractall(path=spam_path)\n        tar_bz2_file.close()\n\nfetch_spam_data()\n\nHAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\nSPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\nham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]\n\nlen(ham_filenames)\n\nlen(spam_filenames)\n\nimport email\nimport email.policy\n\ndef load_email(is_spam, filename, spam_path=SPAM_PATH):\n    directory = \"spam\" if is_spam else \"easy_ham\"\n    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n\nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n\nprint(ham_emails[1].get_content().strip())\n\nprint(spam_emails[6].get_content().strip())\n\ndef get_email_structure(email):\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ]))\n    else:\n        return email.get_content_type()\n\nfrom collections import Counter\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures\n\nstructures_counter(ham_emails).most_common()\n\nstructures_counter(spam_emails).most_common()\n\nfor header, value in spam_emails[0].items():\n    print(header,\":\",value)\n\nspam_emails[0][\"Subject\"]\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(ham_emails + spam_emails, dtype=object)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport re\nfrom html import unescape\n\ndef html_to_plain_text(html):\n    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n    return unescape(text)\n\nhtml_spam_emails = [email for email in X_train[y_train==1]\n                    if get_email_structure(email) == \"text/html\"]\nsample_html_spam = html_spam_emails[7]\nprint(sample_html_spam.get_content().strip()[:1000], \"...\")\n\nprint(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")\n\ndef email_to_text(email):\n    html = None\n    for part in email.walk():\n        ctype = part.get_content_type()\n        if not ctype in (\"text/plain\", \"text/html\"):\n            continue\n        try:\n            content = part.get_content()\n        except: # in case of encoding issues\n            content = str(part.get_payload())\n        if ctype == \"text/plain\":\n            return content\n        else:\n            html = content\n    if html:\n        return html_to_plain_text(html)\n\nprint(email_to_text(sample_html_spam)[:100], \"...\")\n\ntry:\n    import nltk\n\n    stemmer = nltk.PorterStemmer()\n    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n        print(word, \"=>\", stemmer.stem(word))\nexcept ImportError:\n    print(\"Error: stemming requires the NLTK module.\")\n    stemmer = None\n\n# if running this notebook on Colab or Kaggle, we just pip install urlextract\nif IS_COLAB or IS_KAGGLE:\n    %pip install -q -U urlextract\n\ntry:\n    import urlextract # may require an Internet connection to download root domain names\n    \n    url_extractor = urlextract.URLExtract()\n    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\nexcept ImportError:\n    print(\"Error: replacing URLs requires the urlextract module.\")\n    url_extractor = None\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n                 replace_urls=True, replace_numbers=True, stemming=True):\n        self.strip_headers = strip_headers\n        self.lower_case = lower_case\n        self.remove_punctuation = remove_punctuation\n        self.replace_urls = replace_urls\n        self.replace_numbers = replace_numbers\n        self.stemming = stemming\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_transformed = []\n        for email in X:\n            text = email_to_text(email) or \"\"\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_urls and url_extractor is not None:\n                urls = list(set(url_extractor.find_urls(text)))\n                urls.sort(key=lambda url: len(url), reverse=True)\n                for url in urls:\n                    text = text.replace(url, \" URL \")\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            word_counts = Counter(text.split())\n            if self.stemming and stemmer is not None:\n                stemmed_word_counts = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_counts[stemmed_word] += count\n                word_counts = stemmed_word_counts\n            X_transformed.append(word_counts)\n        return np.array(X_transformed)\n\nX_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\nX_few_wordcounts\n\nfrom scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10)\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n\nvocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\nX_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\nX_few_vectors\n\nX_few_vectors.toarray()\n\nvocab_transformer.vocabulary_\n\nfrom sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\n\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nscore.mean()\n\nfrom sklearn.metrics import precision_score, recall_score\n\nX_test_transformed = preprocess_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\nlog_clf.fit(X_train_transformed, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred))) END"}
{"prompt": "The iris dataset\n\nQuestion: Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.\n\nCode:", "completion": " from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nC = 5\nalpha = 1 / (C * len(X))\n\nlin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\nsvm_clf = SVC(kernel=\"linear\", C=C)\nsgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n                        max_iter=1000, tol=1e-3, random_state=42)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlin_clf.fit(X_scaled, y)\nsvm_clf.fit(X_scaled, y)\nsgd_clf.fit(X_scaled, y)\n\nprint(\"LinearSVC:                   \", lin_clf.intercept_, lin_clf.coef_)\nprint(\"SVC:                         \", svm_clf.intercept_, svm_clf.coef_)\nprint(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)\n\n# Compute the slope and bias of each decision boundary\nw1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1]\nb1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1]\nw2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1]\nb2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1]\nw3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1]\nb3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1]\n\n# Transform the decision boundary lines back to the original scale\nline1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])\nline2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])\nline3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])\n\n# Plot all three decision boundaries\nplt.figure(figsize=(11, 4))\nplt.plot(line1[:, 0], line1[:, 1], \"k:\", label=\"LinearSVC\")\nplt.plot(line2[:, 0], line2[:, 1], \"b--\", linewidth=2, label=\"SVC\")\nplt.plot(line3[:, 0], line3[:, 1], \"r-\", label=\"SGDClassifier\")\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\") # label=\"Iris versicolor\"\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\") # label=\"Iris setosa\"\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper center\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\nplt.show() END"}
{"prompt": "The MNIST dataset\n\nQuestion: Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one - versus - all to classify all 10 digits.You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach ?\n\nCode:", "completion": " from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n\nX = mnist[\"data\"]\ny = mnist[\"target\"].astype(np.uint8)\n\nX_train = X[:60000]\ny_train = y[:60000]\nX_test = X[60000:]\ny_test = y[60000:]\n\nlin_clf = LinearSVC(random_state=42)\nlin_clf.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = lin_clf.predict(X_train)\naccuracy_score(y_train, y_pred)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\nX_test_scaled = scaler.transform(X_test.astype(np.float32))\n\nlin_clf = LinearSVC(random_state=42)\nlin_clf.fit(X_train_scaled, y_train)\n\ny_pred = lin_clf.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)\n\nsvm_clf = SVC(gamma=\"scale\")\nsvm_clf.fit(X_train_scaled[:10000], y_train[:10000])\n\ny_pred = svm_clf.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\nrnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])\n\nrnd_search_cv.best_estimator_\n\nrnd_search_cv.best_score_\n\nrnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\naccuracy_score(y_test, y_pred) END"}
{"prompt": "The California Housing dataset in scikit\n\nQuestion: Train an SVM regressor on the California housing dataset.\n\nCode:", "completion": " from sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing()\nX = housing[\"data\"]\ny = housing[\"target\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nfrom sklearn.svm import LinearSVR\n\nlin_svr = LinearSVR(random_state=42)\nlin_svr.fit(X_train_scaled, y_train)\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nmse\n\nnp.sqrt(mse)\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\nrnd_search_cv.fit(X_train_scaled, y_train)\n\nrnd_search_cv.best_estimator_\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nnp.sqrt(mse)\n\ny_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nnp.sqrt(mse) END"}
{"prompt": "The MNIST dataset in scikit\n\nQuestion: Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set(e.g., use 50, 000 instances for training, 10,000 for validation, and 10, 000 for testing).Then train various classifiers, such as a Random Forest classifier, an Extra - Trees classifier, and an SVM.Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier.Once you have found one, try it on the test set.How much better does it perform compared to the individual classifiers ? \n\nCode:", "completion": " from sklearn.model_selection import train_test_split\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(\n    mnist.data, mnist.target, test_size=10000, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=10000, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\n\nrandom_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nextra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\nsvm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\nmlp_clf = MLPClassifier(random_state=42)\n\nestimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\nfor estimator in estimators:\n    print(\"Training the\", estimator)\n    estimator.fit(X_train, y_train)\n\n[estimator.score(X_val, y_val) for estimator in estimators]\n\nfrom sklearn.ensemble import VotingClassifier\n\nnamed_estimators = [\n    (\"random_forest_clf\", random_forest_clf),\n    (\"extra_trees_clf\", extra_trees_clf),\n    (\"svm_clf\", svm_clf),\n    (\"mlp_clf\", mlp_clf),\n]\n\nvoting_clf = VotingClassifier(named_estimators)\n\nvoting_clf.fit(X_train, y_train)\n\nvoting_clf.score(X_val, y_val)\n\n[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]\n\nvoting_clf.set_params(svm_clf=None)\n\nvoting_clf.estimators\n\nvoting_clf.estimators_\n\ndel voting_clf.estimators_[2]\n\nvoting_clf.score(X_val, y_val)\n\nvoting_clf.voting = \"soft\"\n\nvoting_clf.score(X_val, y_val)\n\nvoting_clf.voting = \"hard\"\nvoting_clf.score(X_test, y_test)\n\n[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_] END"}
{"prompt": "The MNIST dataset in scikit\n\nQuestion: Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image\u2019 s class.Train a classifier on this new training set.Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble!Now let\u2019 s evaluate the ensemble on the test set.For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble\u2019 s pre\u2010 dictions.How does it compare to the voting classifier you trained earlier ? \n\nCode:", "completion": " X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict(X_val)\n\nX_val_predictions\n\nrnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\nrnd_forest_blender.fit(X_val_predictions, y_val)\n\nrnd_forest_blender.oob_score_\n\nX_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict(X_test)\n\ny_pred = rnd_forest_blender.predict(X_test_predictions)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred) END"}
{"prompt": "The Olivetti faces dataset in scikit,\n\nQuestion: The classic Olivetti faces dataset contains 400 grayscale 64 \u00d7 64\u2013pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the sklearn.datasets.fetch_olivetti_faces() function.\n\nCode:", "completion": " from sklearn.datasets import fetch_olivetti_faces\n\nolivetti = fetch_olivetti_faces()\n\nprint(olivetti.DESCR)\n\nolivetti.target\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\ntrain_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target))\nX_train_valid = olivetti.data[train_valid_idx]\ny_train_valid = olivetti.target[train_valid_idx]\nX_test = olivetti.data[test_idx]\ny_test = olivetti.target[test_idx]\n\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)\ntrain_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))\nX_train = X_train_valid[train_idx]\ny_train = y_train_valid[train_idx]\nX_valid = X_train_valid[valid_idx]\ny_valid = y_train_valid[valid_idx]\n\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)\nprint(X_test.shape, y_test.shape)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.99)\nX_train_pca = pca.fit_transform(X_train)\nX_valid_pca = pca.transform(X_valid)\nX_test_pca = pca.transform(X_test)\n\npca.n_components_\n\nfrom sklearn.cluster import KMeans\n\nk_range = range(5, 150, 5)\nkmeans_per_k = []\nfor k in k_range:\n    print(\"k={}\".format(k))\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca)\n    kmeans_per_k.append(kmeans)\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = [silhouette_score(X_train_pca, model.labels_)\n                     for model in kmeans_per_k]\nbest_index = np.argmax(silhouette_scores)\nbest_k = k_range[best_index]\nbest_score = silhouette_scores[best_index]\n\nplt.figure(figsize=(8, 3))\nplt.plot(k_range, silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Silhouette score\", fontsize=14)\nplt.plot(best_k, best_score, \"rs\")\nplt.show()\n\nbest_k\n\ninertias = [model.inertia_ for model in kmeans_per_k]\nbest_inertia = inertias[best_index]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(k_range, inertias, \"bo-\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)\nplt.plot(best_k, best_inertia, \"rs\")\nplt.show()\n\nbest_model = kmeans_per_k[best_index]\n\ndef plot_faces(faces, labels, n_cols=5):\n    faces = faces.reshape(-1, 64, 64)\n    n_rows = (len(faces) - 1) // n_cols + 1\n    plt.figure(figsize=(n_cols, n_rows * 1.1))\n    for index, (face, label) in enumerate(zip(faces, labels)):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(face, cmap=\"gray\")\n        plt.axis(\"off\")\n        plt.title(label)\n    plt.show()\n\nfor cluster_id in np.unique(best_model.labels_):\n    print(\"Cluster\", cluster_id)\n    in_cluster = best_model.labels_==cluster_id\n    faces = X_train[in_cluster]\n    labels = y_train[in_cluster]\n    plot_faces(faces, labels) END"}
{"prompt": "The Olivetti faces dataset in scikit.\n\nQuestion: Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set\n\nCode:", "completion": " from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_pca, y_train)\nclf.score(X_valid_pca, y_valid)\n\nX_train_reduced = best_model.transform(X_train_pca)\nX_valid_reduced = best_model.transform(X_valid_pca)\nX_test_reduced = best_model.transform(X_test_pca)\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_reduced, y_train)\n    \nclf.score(X_valid_reduced, y_valid)\n\nfrom sklearn.pipeline import Pipeline\n\nfor n_clusters in k_range:\n    pipeline = Pipeline([\n        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42)),\n        (\"forest_clf\", RandomForestClassifier(n_estimators=150, random_state=42))\n    ])\n    pipeline.fit(X_train_pca, y_train)\n    print(n_clusters, pipeline.score(X_valid_pca, y_valid))\n\nX_train_extended = np.c_[X_train_pca, X_train_reduced]\nX_valid_extended = np.c_[X_valid_pca, X_valid_reduced]\nX_test_extended = np.c_[X_test_pca, X_test_reduced]\n\nclf = RandomForestClassifier(n_estimators=150, random_state=42)\nclf.fit(X_train_extended, y_train)\nclf.score(X_valid_extended, y_valid) END"}
{"prompt": "The Olivetti faces dataset in scikit.\n\nQuestion: a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function. b.Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset.You can load it with keras.datasets.cifar10.load_data().The dataset is composed of 60,000 32\u00d7 32\u2013 pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you 'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model' s architecture or hyperparameters. c.Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before ? Does it produce a better model ? How does it affect training speed ? d.Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self - normalizes(i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). e.Try regularizing the model with alpha dropout.Then, without retraining your model, see if you can achieve better accuracy using MC Dropout. f.Retrain your model using 1 cycle scheduling and see if it improves training speed and model accuracy. \n\nCode:", "completion": " keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 activation=\"elu\",\n                                 kernel_initializer=\"he_normal\"))\n\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-5)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\nX_train = X_train_full[5000:]\ny_train = y_train_full[5000:]\nX_valid = X_train_full[:5000]\ny_valid = y_train_full[:5000]\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\nrun_index = 1 # increment every time you train the model\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\n%tensorboard --logdir=./my_cifar10_logs --port=6006\n\nmodel.fit(X_train, y_train, epochs=100,\n          validation_data=(X_valid, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_model.h5\")\nmodel.evaluate(X_valid, y_valid)\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nmodel.add(keras.layers.BatchNormalization())\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation(\"elu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\nrun_index = 1 # increment every time you train the model\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nmodel.fit(X_train, y_train, epochs=100,\n          validation_data=(X_valid, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_bn_model.h5\")\nmodel.evaluate(X_valid, y_valid)\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=7e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\nrun_index = 1 # increment every time you train the model\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n          validation_data=(X_valid_scaled, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_selu_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\nmodel = keras.models.load_model(\"my_cifar10_selu_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(learning_rate=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\nrun_index = 1 # increment every time you train the model\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n          validation_data=(X_valid_scaled, y_valid),\n          callbacks=callbacks)\n\nmodel = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\nmodel.evaluate(X_valid_scaled, y_valid)\n\nclass MCAlphaDropout(keras.layers.AlphaDropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\nmc_model = keras.models.Sequential([\n    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n    for layer in model.layers\n])\n\ndef mc_dropout_predict_probas(mc_model, X, n_samples=10):\n    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n    return np.mean(Y_probas, axis=0)\n\ndef mc_dropout_predict_classes(mc_model, X, n_samples=10):\n    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n    return np.argmax(Y_probas, axis=1)\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ny_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\naccuracy = np.mean(y_pred == y_valid[:, 0])\naccuracy\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nbatch_size = 128\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)\nplt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\n\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.SGD(learning_rate=1e-2)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\nn_epochs = 15\nonecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[onecycle]) END"}
{"prompt": "The fashion MNIST dataset in scikit.\n\nQuestion: a. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Note: for large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would lose a bit of image quality b.Then use tf.data to create an efficient dataset for each set.Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature.Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data. \n\nCode:", "completion": " (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\ntrain_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\ndef create_example(image, label):\n    image_data = tf.io.serialize_tensor(image)\n    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n    return Example(\n        features=Features(\n            feature={\n                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n                \"label\": Feature(int64_list=Int64List(value=[label])),\n            }))\n\nfor image, label in valid_set.take(1):\n    print(create_example(image, label))\n\nfrom contextlib import ExitStack\n\ndef write_tfrecords(name, dataset, n_shards=10):\n    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n             for index in range(n_shards)]\n    with ExitStack() as stack:\n        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n                   for path in paths]\n        for index, (image, label) in dataset.enumerate():\n            shard = index % n_shards\n            example = create_example(image, label)\n            writers[shard].write(example.SerializeToString())\n    return paths\n\ntrain_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\nvalid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\ntest_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)\n\ndef preprocess(tfrecord):\n    feature_descriptions = {\n        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n    }\n    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n    #image = tf.io.decode_jpeg(example[\"image\"])\n    image = tf.reshape(image, shape=[28, 28])\n    return image, example[\"label\"]\n\ndef mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n                  n_parse_threads=5, batch_size=32, cache=True):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)\n\ntrain_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\nvalid_set = mnist_dataset(valid_filepaths)\ntest_set = mnist_dataset(test_filepaths)\n\nfor X, y in train_set.take(1):\n    for i in range(5):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow(X[i].numpy(), cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.title(str(y[i].numpy()))\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass Standardization(keras.layers.Layer):\n    def adapt(self, data_sample):\n        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n    def call(self, inputs):\n        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n\nstandardization = Standardization(input_shape=[28, 28])\n# or perhaps soon:\n#standardization = keras.layers.Normalization()\n\nsample_image_batches = train_set.take(100).map(lambda image, label: image)\nsample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n                               axis=0).astype(np.float32)\nstandardization.adapt(sample_images)\n\nmodel = keras.models.Sequential([\n    standardization,\n    keras.layers.Flatten(),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"nadam\", metrics=[\"accuracy\"])\n\nfrom datetime import datetime\nlogs = os.path.join(os.curdir, \"my_logs\",\n                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n\ntensorboard_cb = tf.keras.callbacks.TensorBoard(\n    log_dir=logs, histogram_freq=1, profile_batch=10)\n\nmodel.fit(train_set, epochs=5, validation_data=valid_set,\n          callbacks=[tensorboard_cb])\n\n%load_ext tensorboard\n%tensorboard --logdir=./my_logs --port=6006 END"}
{"prompt": "The Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. Found at http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n\nQuestion: a. Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise. b.Split the test set into a validation set(15, 000) and a test set(10, 000). c.Use tf.data to create an efficient dataset for each set. d.Create a binary classification model, using a TextVectorization layer to preprocess each review.If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces.You should use a lookup table to output word indices, which must be prepared in the adapt() method. e.Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words(see Chapter 16).This rescaled mean embedding can then be passed to the rest of your model. f.Train the model and see what accuracy you get.Try to optimize your pipelines to make training as fast as possible. g.Use TFDS to load the same dataset more easily: tfds.load('imdb_reviews') \n\nCode:", "completion": " from pathlib import Path\n\nDOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\nFILENAME = \"aclImdb_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\npath = Path(filepath).parent / \"aclImdb\"\npath\n\nfor name, subdirs, files in os.walk(path):\n    indent = len(Path(name).parts) - len(path.parts)\n    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n    for index, filename in enumerate(sorted(files)):\n        if index == 3:\n            print(\"    \" * (indent + 1) + \"...\")\n            break\n        print(\"    \" * (indent + 1) + filename)\n\ndef review_paths(dirpath):\n    return [str(path) for path in dirpath.glob(\"*.txt\")]\n\ntrain_pos = review_paths(path / \"train\" / \"pos\")\ntrain_neg = review_paths(path / \"train\" / \"neg\")\ntest_valid_pos = review_paths(path / \"test\" / \"pos\")\ntest_valid_neg = review_paths(path / \"test\" / \"neg\")\n\nlen(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)\n\nnp.random.shuffle(test_valid_pos)\n\ntest_pos = test_valid_pos[:5000]\ntest_neg = test_valid_neg[:5000]\nvalid_pos = test_valid_pos[5000:]\nvalid_neg = test_valid_neg[5000:]\n\ndef imdb_dataset(filepaths_positive, filepaths_negative):\n    reviews = []\n    labels = []\n    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n        for filepath in filepaths:\n            with open(filepath) as review_file:\n                reviews.append(review_file.read())\n            labels.append(label)\n    return tf.data.Dataset.from_tensor_slices(\n        (tf.constant(reviews), tf.constant(labels)))\n\nfor X, y in imdb_dataset(train_pos, train_neg).take(3):\n    print(X)\n    print(y)\n    print()\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass\n\ndef imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n                                          num_parallel_reads=n_read_threads)\n    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n                                          num_parallel_reads=n_read_threads)\n    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass\n\n%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass\n\nbatch_size = 32\n\ntrain_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\nvalid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\ntest_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)\n\ndef preprocess(X_batch, n_words=50):\n    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n    Z = tf.strings.substr(X_batch, 0, 300)\n    Z = tf.strings.lower(Z)\n    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n    Z = tf.strings.split(Z)\n    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n\nX_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\npreprocess(X_example)\n\nfrom collections import Counter\n\ndef get_vocabulary(data_sample, max_size=1000):\n    preprocessed_reviews = preprocess(data_sample).numpy()\n    counter = Counter()\n    for words in preprocessed_reviews:\n        for word in words:\n            if word != b\"<pad>\":\n                counter[word] += 1\n    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n\nget_vocabulary(X_example)\n\nclass TextVectorization(keras.layers.Layer):\n    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.max_vocabulary_size = max_vocabulary_size\n        self.n_oov_buckets = n_oov_buckets\n\n    def adapt(self, data_sample):\n        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n        words = tf.constant(self.vocab)\n        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n        \n    def call(self, inputs):\n        preprocessed_inputs = preprocess(inputs)\n        return self.table.lookup(preprocessed_inputs)\n\ntext_vectorization = TextVectorization()\n\ntext_vectorization.adapt(X_example)\ntext_vectorization(X_example)\n\nmax_vocabulary_size = 1000\nn_oov_buckets = 100\n\nsample_review_batches = train_set.map(lambda review, label: review)\nsample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n                                axis=0)\n\ntext_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n                                       input_shape=[])\ntext_vectorization.adapt(sample_reviews)\n\ntext_vectorization(X_example)\n\ntext_vectorization.vocab[:10]\n\nsimple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\ntf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)\n\nclass BagOfWords(keras.layers.Layer):\n    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.n_tokens = n_tokens\n    def call(self, inputs):\n        one_hot = tf.one_hot(inputs, self.n_tokens)\n        return tf.reduce_sum(one_hot, axis=1)[:, 1:]\n\nbag_of_words = BagOfWords(n_tokens=4)\nbag_of_words(simple_example)\n\nn_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\nbag_of_words = BagOfWords(n_tokens)\n\nmodel = keras.models.Sequential([\n    text_vectorization,\n    bag_of_words,\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)\n\ndef compute_mean_embedding(inputs):\n    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n\nanother_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\ncompute_mean_embedding(another_example)\n\ntf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)\n\ntf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)\n\nembedding_size = 20\n\nmodel = keras.models.Sequential([\n    text_vectorization,\n    keras.layers.Embedding(input_dim=n_tokens,\n                           output_dim=embedding_size,\n                           mask_zero=True), # <pad> tokens => zero vectors\n    keras.layers.Lambda(compute_mean_embedding),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)\n\nimport tensorflow_datasets as tfds\n\ndatasets = tfds.load(name=\"imdb_reviews\")\ntrain_set, test_set = datasets[\"train\"], datasets[\"test\"]\n\nfor example in train_set.take(1):\n    print(example[\"text\"])\n    print(example[\"label\"]) END"}
{"prompt": "SketchRNN dataset\n\nQuestion: Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets\n\nCode:", "completion": " DOWNLOAD_ROOT = \"http://download.tensorflow.org/data/\"\nFILENAME = \"quickdraw_tutorial_dataset_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\"datasets/quickdraw\",\n                                extract=True)\n\nquickdraw_dir = Path(filepath).parent\ntrain_files = sorted([str(path) for path in quickdraw_dir.glob(\"training.tfrecord-*\")])\neval_files = sorted([str(path) for path in quickdraw_dir.glob(\"eval.tfrecord-*\")])\n\ntrain_files\n\neval_files\n\nwith open(quickdraw_dir / \"eval.tfrecord.classes\") as test_classes_file:\n    test_classes = test_classes_file.readlines()\n    \nwith open(quickdraw_dir / \"training.tfrecord.classes\") as train_classes_file:\n    train_classes = train_classes_file.readlines()\n\nassert train_classes == test_classes\nclass_names = [name.strip().lower() for name in train_classes]\n\nsorted(class_names)\n\ndef parse(data_batch):\n    feature_descriptions = {\n        \"ink\": tf.io.VarLenFeature(dtype=tf.float32),\n        \"shape\": tf.io.FixedLenFeature([2], dtype=tf.int64),\n        \"class_index\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n    }\n    examples = tf.io.parse_example(data_batch, feature_descriptions)\n    flat_sketches = tf.sparse.to_dense(examples[\"ink\"])\n    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), -1, 3])\n    lengths = examples[\"shape\"][:, 0]\n    labels = examples[\"class_index\"][:, 0]\n    return sketches, lengths, labels\n\ndef quickdraw_dataset(filepaths, batch_size=32, shuffle_buffer_size=None,\n                      n_parse_threads=5, n_read_threads=5, cache=False):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n    return dataset.prefetch(1)\n\ntrain_set = quickdraw_dataset(train_files, shuffle_buffer_size=10000)\nvalid_set = quickdraw_dataset(eval_files[:5])\ntest_set = quickdraw_dataset(eval_files[5:])\n\nfor sketches, lengths, labels in train_set.take(1):\n    print(\"sketches =\", sketches)\n    print(\"lengths =\", lengths)\n    print(\"labels =\", labels)\n\ndef draw_sketch(sketch, label=None):\n    origin = np.array([[0., 0., 0.]])\n    sketch = np.r_[origin, sketch]\n    stroke_end_indices = np.argwhere(sketch[:, -1]==1.)[:, 0]\n    coordinates = np.cumsum(sketch[:, :2], axis=0)\n    strokes = np.split(coordinates, stroke_end_indices + 1)\n    title = class_names[label.numpy()] if label is not None else \"Try to guess\"\n    plt.title(title)\n    plt.plot(coordinates[:, 0], -coordinates[:, 1], \"y:\")\n    for stroke in strokes:\n        plt.plot(stroke[:, 0], -stroke[:, 1], \".-\")\n    plt.axis(\"off\")\n\ndef draw_sketches(sketches, lengths, labels):\n    n_sketches = len(sketches)\n    n_cols = 4\n    n_rows = (n_sketches - 1) // n_cols + 1\n    plt.figure(figsize=(n_cols * 3, n_rows * 3.5))\n    for index, sketch, length, label in zip(range(n_sketches), sketches, lengths, labels):\n        plt.subplot(n_rows, n_cols, index + 1)\n        draw_sketch(sketch[:length], label)\n    plt.show()\n\nfor sketches, lengths, labels in train_set.take(1):\n    draw_sketches(sketches, lengths, labels)\n\nlengths = np.concatenate([lengths for _, lengths, _ in train_set.take(1000)])\nplt.hist(lengths, bins=150, density=True)\nplt.axis([0, 200, 0, 0.03])\nplt.xlabel(\"length\")\nplt.ylabel(\"density\")\nplt.show()\n\ndef crop_long_sketches(dataset, max_length=100):\n    return dataset.map(lambda inks, lengths, labels: (inks[:, :max_length], labels))\n\ncropped_train_set = crop_long_sketches(train_set)\ncropped_valid_set = crop_long_sketches(valid_set)\ncropped_test_set = crop_long_sketches(test_set)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv1D(32, kernel_size=5, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(64, kernel_size=5, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(128, kernel_size=3, strides=2, activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(128, return_sequences=True),\n    keras.layers.LSTM(128),\n    keras.layers.Dense(len(class_names), activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=1e-2, clipnorm=1.)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"])\nhistory = model.fit(cropped_train_set, epochs=2,\n                    validation_data=cropped_valid_set)\n\ny_test = np.concatenate([labels for _, _, labels in test_set])\ny_probas = model.predict(test_set)\n\nnp.mean(keras.metrics.sparse_top_k_categorical_accuracy(y_test, y_probas))\n\nn_new = 10\nY_probas = model.predict(sketches)\ntop_k = tf.nn.top_k(Y_probas, k=5)\nfor index in range(n_new):\n    plt.figure(figsize=(3, 3.5))\n    draw_sketch(sketches[index])\n    plt.show()\n    print(\"Top-5 predictions:\".format(index + 1))\n    for k in range(5):\n        class_name = class_names[top_k.indices[index, k]]\n        proba = 100 * top_k.values[index, k]\n        print(\"  {}. {} {:.3f}%\".format(k + 1, class_name, proba))\n    print(\"Answer: {}\".format(class_names[labels[index].numpy()]))\n\nmodel.save(\"my_sketchrnn\") END"}
{"prompt": "The Bach chorales dataset. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played). Train a model\u2014recurrent, convolutional, or both\u2014that can predict the next time step (four notes), given a sequence of time steps from a chorale. Found at https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/jsb_chorales.tgz\n\nQuestion: Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played). Train a model\u2014recurrent, convolutional, or both\u2014that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google's Coconet model, which was used for a nice Google doodle about Bach.\n\nCode:", "completion": " DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\nFILENAME = \"jsb_chorales.tgz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\"datasets/jsb_chorales\",\n                                extract=True)\n\njsb_chorales_dir = Path(filepath).parent\ntrain_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\nvalid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\ntest_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))\n\nimport pandas as pd\n\ndef load_chorales(filepaths):\n    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n\ntrain_chorales = load_chorales(train_files)\nvalid_chorales = load_chorales(valid_files)\ntest_chorales = load_chorales(test_files)\n\ntrain_chorales[0]\n\nnotes = set()\nfor chorales in (train_chorales, valid_chorales, test_chorales):\n    for chorale in chorales:\n        for chord in chorale:\n            notes |= set(chord)\n\nn_notes = len(notes)\nmin_note = min(notes - {0})\nmax_note = max(notes)\n\nassert min_note == 36\nassert max_note == 81\n\nfrom IPython.display import Audio\n\ndef notes_to_frequencies(notes):\n    # Frequency doubles when you go up one octave; there are 12 semi-tones\n    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n    return 2 ** ((np.array(notes) - 69) / 12) * 440\n\ndef frequencies_to_samples(frequencies, tempo, sample_rate):\n    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n    # To reduce click sound at every beat, we round the frequencies to try to\n    # get the samples close to zero at the end of each note.\n    frequencies = np.round(note_duration * frequencies) / note_duration\n    n_samples = int(note_duration * sample_rate)\n    time = np.linspace(0, note_duration, n_samples)\n    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n    # Removing all notes with frequencies \u2264 9 Hz (includes note 0 = silence)\n    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n    return sine_waves.reshape(-1)\n\ndef chords_to_samples(chords, tempo, sample_rate):\n    freqs = notes_to_frequencies(chords)\n    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n                     for melody in freqs.T], axis=0)\n    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n    merged[-n_fade_out_samples:] *= fade_out\n    return merged\n\ndef play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n    if filepath:\n        from scipy.io import wavfile\n        samples = (2**15 * samples).astype(np.int16)\n        wavfile.write(filepath, sample_rate, samples)\n        return display(Audio(filepath))\n    else:\n        return display(Audio(samples, rate=sample_rate))\n\nfor index in range(3):\n    play_chords(train_chorales[index])\n\ndef create_target(batch):\n    X = batch[:, :-1]\n    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n    return X, Y\n\ndef preprocess(window):\n    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n    return tf.reshape(window, [-1]) # convert to arpegio\n\ndef bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n                 window_size=32, window_shift=16, cache=True):\n    def batch_window(window):\n        return window.batch(window_size + 1)\n\n    def to_windows(chorale):\n        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n        return dataset.flat_map(batch_window)\n\n    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n    dataset = dataset.flat_map(to_windows).map(preprocess)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(create_target)\n    return dataset.prefetch(1)\n\ntrain_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\nvalid_set = bach_dataset(valid_chorales)\ntest_set = bach_dataset(test_chorales)\n\nn_embedding_dims = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n                           input_shape=[None]),\n    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(256, return_sequences=True),\n    keras.layers.Dense(n_notes, activation=\"softmax\")\n])\n\nmodel.summary()\n\noptimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=20, validation_data=valid_set)\n\nmodel.save(\"my_bach_model.h5\")\nmodel.evaluate(test_set)\n\ndef generate_chorale(model, seed_chords, length):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            #next_note = model.predict_classes(arpegio)[:1, -1:]\n            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\nseed_chords = test_chorales[2][:8]\nplay_chords(seed_chords, amplitude=0.2)\n\nnew_chorale = generate_chorale(model, seed_chords, 56)\nplay_chords(new_chorale)\n\ndef generate_chorale_v2(model, seed_chords, length, temperature=1):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            next_note_probas = model.predict(arpegio)[0, -1:]\n            rescaled_logits = tf.math.log(next_note_probas) / temperature\n            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\nnew_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\nplay_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")\n\nnew_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\nplay_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")\n\nnew_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\nplay_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")\n\nplay_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\") END"}
{"prompt": "MNIST or CIFAR10 datasets\n\nQuestion: Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as CIFAR10 if you want a bigger challenge. Regardless of the dataset you're using, follow these steps: Split the dataset into a training set and a test set.Train a deep denoising autoencoder on the full training set. Check that the images are fairly well reconstructed.Visualize the images that most activate each neuron in the coding layer. Build a classification DNN, reusing the lower layers of the autoencoder.Train it using only 500 images from the training set.Does it perform better with or without pretraining ? \n\nCode:", "completion": " DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\nFILENAME = \"jsb_chorales.tgz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\"datasets/jsb_chorales\",\n                                extract=True)\n\njsb_chorales_dir = Path(filepath).parent\ntrain_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\nvalid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\ntest_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))\n\nimport pandas as pd\n\ndef load_chorales(filepaths):\n    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n\ntrain_chorales = load_chorales(train_files)\nvalid_chorales = load_chorales(valid_files)\ntest_chorales = load_chorales(test_files)\n\ntrain_chorales[0]\n\nnotes = set()\nfor chorales in (train_chorales, valid_chorales, test_chorales):\n    for chorale in chorales:\n        for chord in chorale:\n            notes |= set(chord)\n\nn_notes = len(notes)\nmin_note = min(notes - {0})\nmax_note = max(notes)\n\nassert min_note == 36\nassert max_note == 81\n\nfrom IPython.display import Audio\n\ndef notes_to_frequencies(notes):\n    # Frequency doubles when you go up one octave; there are 12 semi-tones\n    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n    return 2 ** ((np.array(notes) - 69) / 12) * 440\n\ndef frequencies_to_samples(frequencies, tempo, sample_rate):\n    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n    # To reduce click sound at every beat, we round the frequencies to try to\n    # get the samples close to zero at the end of each note.\n    frequencies = np.round(note_duration * frequencies) / note_duration\n    n_samples = int(note_duration * sample_rate)\n    time = np.linspace(0, note_duration, n_samples)\n    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n    # Removing all notes with frequencies \u2264 9 Hz (includes note 0 = silence)\n    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n    return sine_waves.reshape(-1)\n\ndef chords_to_samples(chords, tempo, sample_rate):\n    freqs = notes_to_frequencies(chords)\n    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n                     for melody in freqs.T], axis=0)\n    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n    merged[-n_fade_out_samples:] *= fade_out\n    return merged\n\ndef play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n    if filepath:\n        from scipy.io import wavfile\n        samples = (2**15 * samples).astype(np.int16)\n        wavfile.write(filepath, sample_rate, samples)\n        return display(Audio(filepath))\n    else:\n        return display(Audio(samples, rate=sample_rate))\n\nfor index in range(3):\n    play_chords(train_chorales[index])\n\ndef create_target(batch):\n    X = batch[:, :-1]\n    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n    return X, Y\n\ndef preprocess(window):\n    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n    return tf.reshape(window, [-1]) # convert to arpegio\n\ndef bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n                 window_size=32, window_shift=16, cache=True):\n    def batch_window(window):\n        return window.batch(window_size + 1)\n\n    def to_windows(chorale):\n        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n        return dataset.flat_map(batch_window)\n\n    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n    dataset = dataset.flat_map(to_windows).map(preprocess)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(create_target)\n    return dataset.prefetch(1)\n\ntrain_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\nvalid_set = bach_dataset(valid_chorales)\ntest_set = bach_dataset(test_chorales)\n\nn_embedding_dims = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n                           input_shape=[None]),\n    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(256, return_sequences=True),\n    keras.layers.Dense(n_notes, activation=\"softmax\")\n])\n\nmodel.summary()\n\noptimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=20, validation_data=valid_set)\n\nmodel.save(\"my_bach_model.h5\")\nmodel.evaluate(test_set)\n\ndef generate_chorale(model, seed_chords, length):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            #next_note = model.predict_classes(arpegio)[:1, -1:]\n            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\nseed_chords = test_chorales[2][:8]\nplay_chords(seed_chords, amplitude=0.2)\n\nnew_chorale = generate_chorale(model, seed_chords, 56)\nplay_chords(new_chorale)\n\ndef generate_chorale_v2(model, seed_chords, length, temperature=1):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [1, -1])\n    for chord in range(length):\n        for note in range(4):\n            next_note_probas = model.predict(arpegio)[0, -1:]\n            rescaled_logits = tf.math.log(next_note_probas) / temperature\n            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n            arpegio = tf.concat([arpegio, next_note], axis=1)\n    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n    return tf.reshape(arpegio, shape=[-1, 4])\n\nnew_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\nplay_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")\n\nnew_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\nplay_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")\n\nnew_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\nplay_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")\n\nplay_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\") END"}
{"prompt": "Housing data https://raw.githubusercontent.com/ageron/handson-ml2/master/\n\nQuestion: Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don\u2019t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?\n\nCode:", "completion": " from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_prepared, housing_labels)\n\nnegative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\ngrid_search.best_params_\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n# see https://docs.scipy.org/doc/scipy/reference/stats.html\n# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n\n# Note: gamma is ignored when kernel is \"linear\"\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 200000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\n\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse\n\nrnd_search.best_params_\n\nexpon_distrib = expon(scale=1.)\nsamples = expon_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Exponential distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\nreciprocal_distrib = reciprocal(20, 200000)\nsamples = reciprocal_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Reciprocal distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]\n\nk = 5\n\ntop_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices\n\nnp.array(attributes)[top_k_feature_indices]\n\nsorted(zip(feature_importances, attributes), reverse=True)[:k]\n\npreparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n\nhousing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)\n\nhousing_prepared_top_k_features[0:3]\n\nhousing_prepared[0:3, top_k_feature_indices]\n\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n\nprepare_select_and_predict_pipeline.fit(housing, housing_labels)\n\nsome_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\n\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n\nfull_pipeline.named_transformers_[\"cat\"].handle_unknown = 'ignore'\n\nparam_grid = [{\n    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n}]\n\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)\n\ngrid_search_prep.best_params_ END"}
{"prompt": "the usual Iris dataset\n\nQuestion: Implement Batch Gradient Descent with early stopping for Softmax Regression\n(without using Scikit-Learn).\n\nCode:", "completion": " X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nX_with_bias = np.c_[np.ones([len(X), 1]), X]\n\nnp.random.seed(2042)\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\ndef to_one_hot(y):\n    n_classes = y.max() + 1\n    m = len(y)\n    Y_one_hot = np.zeros((m, n_classes))\n    Y_one_hot[np.arange(m), y] = 1\n    return Y_one_hot\n\ny_train[:10]\n\nto_one_hot(y_train[:10])\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = np.sum(exps, axis=1, keepdims=True)\n    return exps / exp_sums\n\nn_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)\n\neta = 0.01\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error)\n    Theta = Theta - eta * gradients\n\nTheta\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\neta = 0.1\nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # regularization hyperparameter\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    if iteration % 500 == 0:\n        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n        loss = xentropy_loss + alpha * l2_loss\n        print(iteration, loss)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\neta = 0.1 \nn_iterations = 5001\nm = len(X_train)\nepsilon = 1e-7\nalpha = 0.1  # regularization hyperparameter\nbest_loss = np.infty\n\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor iteration in range(n_iterations):\n    logits = X_train.dot(Theta)\n    Y_proba = softmax(logits)\n    error = Y_proba - Y_train_one_hot\n    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n    logits = X_valid.dot(Theta)\n    Y_proba = softmax(logits)\n    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n    loss = xentropy_loss + alpha * l2_loss\n    if iteration % 500 == 0:\n        print(iteration, loss)\n    if loss < best_loss:\n        best_loss = loss\n    else:\n        print(iteration - 1, best_loss)\n        print(iteration, loss, \"early stopping!\")\n        break\n\nlogits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n\nx0, x1 = np.meshgrid(\n        np.linspace(0, 8, 500).reshape(-1, 1),\n        np.linspace(0, 3.5, 200).reshape(-1, 1),\n    )\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n\nlogits = X_new_with_bias.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1, fontsize=12)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 7, 0, 3.5])\nplt.show()\n\nlogits = X_test.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy_score = np.mean(y_predict == y_test)\naccuracy_score END"}
{"prompt": "moons dataset\n\nQuestion: Grow a forest.\na. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn\u2019s ShuffleSplit class for this.\nb. Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.\nc. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy\u2019s mode() function for this). This gives you majority-vote predictions over the test set.\nd. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!\n\nCode:", "completion": " from sklearn.model_selection import ShuffleSplit\n\nn_trees = 1000\nn_instances = 100\n\nmini_sets = []\n\nrs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\nfor mini_train_index, mini_test_index in rs.split(X_train):\n    X_mini_train = X_train[mini_train_index]\n    y_mini_train = y_train[mini_train_index]\n    mini_sets.append((X_mini_train, y_mini_train))\n\nfrom sklearn.base import clone\n\nforest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]\n\naccuracy_scores = []\n\nfor tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n    tree.fit(X_mini_train, y_mini_train)\n    \n    y_pred = tree.predict(X_test)\n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n\nnp.mean(accuracy_scores)\n\nY_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n\nfor tree_index, tree in enumerate(forest):\n    Y_pred[tree_index] = tree.predict(X_test)\n\nfrom scipy.stats import mode\n\ny_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n\naccuracy_score(y_test, y_pred_majority_votes.reshape([-1])) END"}
{"prompt": "MNIST\n\nQuestion: Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image\u2019s target class. Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\n\nCode:", "completion": " np.random.seed(42)\n\nm = 10000\nidx = np.random.permutation(60000)[:m]\n\nX = mnist['data'][idx]\ny = mnist['target'][idx]\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)\n\nplt.figure(figsize=(13,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()\n\nplt.figure(figsize=(9,9))\ncmap = mpl.cm.get_cmap(\"jet\")\nfor digit in (2, 3, 5):\n    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\nidx = (y == 2) | (y == 3) | (y == 5) \nX_subset = X[idx]\ny_subset = y[idx]\n\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)\n\nplt.figure(figsize=(9,9))\nfor digit in (2, 3, 5):\n    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)])\nplt.axis('off')\nplt.show()\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\ndef plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n    # Let's scale the input features so that they range from 0 to 1\n    X_normalized = MinMaxScaler().fit_transform(X)\n    # Now we create the list of coordinates of the digits plotted so far.\n    # We pretend that one is already plotted far away at the start, to\n    # avoid `if` statements in the loop below\n    neighbors = np.array([[10., 10.]])\n    # The rest should be self-explanatory\n    plt.figure(figsize=figsize)\n    cmap = mpl.cm.get_cmap(\"jet\")\n    digits = np.unique(y)\n    for digit in digits:\n        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])\n    plt.axis(\"off\")\n    ax = plt.gcf().gca()  # get current axes in current figure\n    for index, image_coord in enumerate(X_normalized):\n        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n        if closest_distance > min_distance:\n            neighbors = np.r_[neighbors, [image_coord]]\n            if images is None:\n                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n                         color=cmap(y[index] / 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n            else:\n                image = images[index].reshape(28, 28)\n                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                ax.add_artist(imagebox)\n\nplot_digits(X_reduced, y)\n\nplot_digits(X_reduced, y, images=X, figsize=(35, 25))\n\nplot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))\n\nfrom sklearn.decomposition import PCA\nimport time\n\nt0 = time.time()\nX_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"PCA took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_reduced, y)\nplt.show()\n\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nt0 = time.time()\nX_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"LLE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lle_reduced, y)\nplt.show()\n\nfrom sklearn.pipeline import Pipeline\n\npca_lle = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_lle_reduced = pca_lle.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+LLE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_lle_reduced, y)\nplt.show()\n\nfrom sklearn.manifold import MDS\n\nm = 2000\nt0 = time.time()\nX_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])\nt1 = time.time()\nprint(\"MDS took {:.1f}s (on just 2,000 MNIST images instead of 10,000).\".format(t1 - t0))\nplot_digits(X_mds_reduced, y[:m])\nplt.show()\n\nfrom sklearn.pipeline import Pipeline\n\npca_mds = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"mds\", MDS(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_mds_reduced = pca_mds.fit_transform(X[:2000])\nt1 = time.time()\nprint(\"PCA+MDS took {:.1f}s (on 2,000 MNIST images).\".format(t1 - t0))\nplot_digits(X_pca_mds_reduced, y[:2000])\nplt.show()\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nt0 = time.time()\nX_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nt1 = time.time()\nprint(\"LDA took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_lda_reduced, y, figsize=(12,12))\nplt.show()\n\nfrom sklearn.manifold import TSNE\n\nt0 = time.time()\nX_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"t-SNE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_tsne_reduced, y)\nplt.show()\n\npca_tsne = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"tsne\", TSNE(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_tsne_reduced = pca_tsne.fit_transform(X)\nt1 = time.time()\nprint(\"PCA+t-SNE took {:.1f}s.\".format(t1 - t0))\nplot_digits(X_pca_tsne_reduced, y)\nplt.show() END"}
{"prompt": "MNIST\n\nQuestion: Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, use early stopping, plot learning curves using TensorBoard, and so on).\n\nCode:", "completion": " (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n\nX_train_full.shape\n\nX_train_full.dtype\n\nX_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test / 255.\n\nplt.imshow(X_train[0], cmap=\"binary\")\nplt.axis('off')\nplt.show()\n\ny_train\n\nX_valid.shape\n\nX_test.shape\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(y_train[index], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n\nK = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nexpon_lr = ExponentialLearningRate(factor=1.005)\n\nhistory = model.fit(X_train, y_train, epochs=1,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[expon_lr])\n\nplt.plot(expon_lr.rates, expon_lr.losses)\nplt.gca().set_xscale('log')\nplt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\nplt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\nplt.grid()\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n              metrics=[\"accuracy\"])\n\nrun_index = 1 # increment this at every run\nrun_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\nrun_logdir\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n\nhistory = model.fit(X_train, y_train, epochs=100,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])\n\nmodel = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\nmodel.evaluate(X_test, y_test) END"}
{"prompt": "california_housing  from sklearn.datasets import fetch_california_housing\n\nQuestion: Implement a custom layer that performs Layer Normalization:\na. The build() method should define two trainable weights \u03b1 and \u03b2, both of shape input_shape[-1:] and data type tf.float32. \u03b1 should be initialized with 1s, and \u03b2 with 0s.\nb. The call() method should compute the mean \u03bc and standard deviation \u03c3 of each instance's features. For this, you can use tf.nn.moments(inputs, axes=-1, keepdims=True), which returns the mean \u03bc and the variance \u03c32 of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return \u03b1\u2297(X - \u03bc)/(\u03c3 + \u03b5) + \u03b2, where \u2297 represents itemwise multiplication (*) and \u03b5 is a smoothing term (small constant to avoid division by zero, e.g., 0.001).\nc. Ensure that your custom layer produces the same (or very nearly the same) output as the keras.layers.LayerNormalization layer.\n\nCode:", "completion": " class LayerNormalization(keras.layers.Layer):\n    def __init__(self, eps=0.001, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n\n    def build(self, batch_input_shape):\n        self.alpha = self.add_weight(\n            name=\"alpha\", shape=batch_input_shape[-1:],\n            initializer=\"ones\")\n        self.beta = self.add_weight(\n            name=\"beta\", shape=batch_input_shape[-1:],\n            initializer=\"zeros\")\n        super().build(batch_input_shape) # must be at the end\n\n    def call(self, X):\n        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"eps\": self.eps}\n\nX = X_train.astype(np.float32)\n\ncustom_layer_norm = LayerNormalization()\nkeras_layer_norm = keras.layers.LayerNormalization()\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X)))\n\nrandom_alpha = np.random.rand(X.shape[-1])\nrandom_beta = np.random.rand(X.shape[-1])\n\ncustom_layer_norm.set_weights([random_alpha, random_beta])\nkeras_layer_norm.set_weights([random_alpha, random_beta])\n\ntf.reduce_mean(keras.losses.mean_absolute_error(\n    keras_layer_norm(X), custom_layer_norm(X))) END"}
{"prompt": "Fashion MNIST\n\nQuestion: Train a model using a custom training loop to tackle the Fashion MNIST dataset.\na. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\nb. Try using a different optimizer with a different learning rate for the upper layers and the lower layers.\n\nCode:", "completion": " (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test.astype(np.float32) / 255.\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape() as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                gradients = tape.gradient(loss, model.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states()\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nlower_layers = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"relu\"),\n])\nupper_layers = keras.models.Sequential([\n    keras.layers.Dense(10, activation=\"softmax\"),\n])\nmodel = keras.models.Sequential([\n    lower_layers, upper_layers\n])\n\nlower_optimizer = keras.optimizers.SGD(learning_rate=1e-4)\nupper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\n\nwith trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n    for epoch in epochs:\n        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n            for step in steps:\n                X_batch, y_batch = random_batch(X_train, y_train)\n                with tf.GradientTape(persistent=True) as tape:\n                    y_pred = model(X_batch)\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    loss = tf.add_n([main_loss] + model.losses)\n                for layers, optimizer in ((lower_layers, lower_optimizer),\n                                          (upper_layers, upper_optimizer)):\n                    gradients = tape.gradient(loss, layers.trainable_variables)\n                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n                del tape\n                for variable in model.variables:\n                    if variable.constraint is not None:\n                        variable.assign(variable.constraint(variable))                    \n                status = OrderedDict()\n                mean_loss(loss)\n                status[\"loss\"] = mean_loss.result().numpy()\n                for metric in metrics:\n                    metric(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n                steps.set_postfix(status)\n            y_pred = model(X_valid)\n            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n                tf.constant(y_valid, dtype=np.float32), y_pred))\n            steps.set_postfix(status)\n        for metric in [mean_loss] + metrics:\n            metric.reset_states() END"}
{"prompt": "MNIST\n\nQuestion: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n\nCode:", "completion": " (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_full = X_train_full / 255.\nX_test = X_test / 255.\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\nX_train = X_train[..., np.newaxis]\nX_valid = X_valid[..., np.newaxis]\nX_test = X_test[..., np.newaxis]\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\nmodel.evaluate(X_test, y_test) END"}
{"prompt": "generated dataset\n\nQuestion: Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't.\n\nCode:", "completion": " default_reber_grammar = [\n    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n    [(\"T\", 3), (\"V\", 5)], # and so on...\n    [(\"X\", 3), (\"S\", 6)],\n    [(\"P\", 4), (\"V\", 6)],\n    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n\nembedded_reber_grammar = [\n    [(\"B\", 1)],\n    [(\"T\", 2), (\"P\", 3)],\n    [(default_reber_grammar, 4)],\n    [(default_reber_grammar, 5)],\n    [(\"T\", 6)],\n    [(\"P\", 6)],\n    [(\"E\", None)]]\n\ndef generate_string(grammar):\n    state = 0\n    output = []\n    while state is not None:\n        index = np.random.randint(len(grammar[state]))\n        production, state = grammar[state][index]\n        if isinstance(production, list):\n            production = generate_string(grammar=production)\n        output.append(production)\n    return \"\".join(output)\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(default_reber_grammar), end=\" \")\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_string(embedded_reber_grammar), end=\" \")\n\nPOSSIBLE_CHARS = \"BEPSTVX\"\n\ndef generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n    good_string = generate_string(grammar)\n    index = np.random.randint(len(good_string))\n    good_char = good_string[index]\n    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n    return good_string[:index] + bad_char + good_string[index + 1:]\n\nnp.random.seed(42)\n\nfor _ in range(25):\n    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")\n\ndef string_to_ids(s, chars=POSSIBLE_CHARS):\n    return [chars.index(c) for c in s]\n\nstring_to_ids(\"BTTTXXVVETE\")\n\ndef generate_dataset(size):\n    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n                    for _ in range(size // 2)]\n    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n                   for _ in range(size - size // 2)]\n    all_strings = good_strings + bad_strings\n    X = tf.ragged.constant(all_strings, ragged_rank=1)\n    y = np.array([[1.] for _ in range(len(good_strings))] +\n                 [[0.] for _ in range(len(bad_strings))])\n    return X, y\n\nnp.random.seed(42)\n\nX_train, y_train = generate_dataset(10000)\nX_valid, y_valid = generate_dataset(2000)\n\nX_train[0]\n\ny_train[0]\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nembedding_size = 5\n\nmodel = keras.models.Sequential([\n    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n    keras.layers.GRU(30),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n\ntest_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\nX_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n\ny_proba = model.predict(X_test)\nprint()\nprint(\"Estimated probability that these are Reber strings:\")\nfor index, string in enumerate(test_strings):\n    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0])) END"}
{"prompt": "openai-gpt\n\nQuestion: Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text.\n\nCode:", "completion": " from transformers import TFOpenAIGPTLMHeadModel\n\nmodel = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\nfrom transformers import OpenAIGPTTokenizer\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n\nprompt_text = \"This royal throne of kings, this sceptred isle\"\nencoded_prompt = tokenizer.encode(prompt_text,\n                                  add_special_tokens=False,\n                                  return_tensors=\"tf\")\nencoded_prompt\n\nnum_sequences = 5\nlength = 40\n\ngenerated_sequences = model.generate(\n    input_ids=encoded_prompt,\n    do_sample=True,\n    max_length=length + len(encoded_prompt[0]),\n    temperature=1.0,\n    top_k=0,\n    top_p=0.9,\n    repetition_penalty=1.0,\n    num_return_sequences=num_sequences,\n)\n\ngenerated_sequences\n\nfor sequence in generated_sequences:\n    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n    print(text)\n    print(\"-\" * 80) END"}
{"prompt": "OpenAI LunarLander-v2 environment\n\nQuestion: Use policy gradients to solve OpenAI Gym's LunarLander-v2 environment. You will need to install the Box2D dependencies (%pip install -U gym[box2d]).\n\nCode:", "completion": " env = gym.make(\"LunarLander-v2\")\n\nenv.observation_space\n\nenv.seed(42)\nobs = env.reset()\nobs\n\nenv.action_space\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nn_inputs = env.observation_space.shape[0]\nn_outputs = env.action_space.n\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n    keras.layers.Dense(32, activation=\"relu\"),\n    keras.layers.Dense(n_outputs, activation=\"softmax\"),\n])\n\ndef lander_play_one_step(env, obs, model, loss_fn):\n    with tf.GradientTape() as tape:\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        loss = tf.reduce_mean(loss_fn(action, probas))\n    grads = tape.gradient(loss, model.trainable_variables)\n    obs, reward, done, info = env.step(action[0, 0].numpy())\n    return obs, reward, done, grads\n\ndef lander_play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n    all_rewards = []\n    all_grads = []\n    for episode in range(n_episodes):\n        current_rewards = []\n        current_grads = []\n        obs = env.reset()\n        for step in range(n_max_steps):\n            obs, reward, done, grads = lander_play_one_step(env, obs, model, loss_fn)\n            current_rewards.append(reward)\n            current_grads.append(grads)\n            if done:\n                break\n        all_rewards.append(current_rewards)\n        all_grads.append(current_grads)\n    return all_rewards, all_grads\n\ndef discount_rewards(rewards, discount_rate):\n    discounted = np.array(rewards)\n    for step in range(len(rewards) - 2, -1, -1):\n        discounted[step] += discounted[step + 1] * discount_rate\n    return discounted\n\ndef discount_and_normalize_rewards(all_rewards, discount_rate):\n    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n                              for rewards in all_rewards]\n    flat_rewards = np.concatenate(all_discounted_rewards)\n    reward_mean = flat_rewards.mean()\n    reward_std = flat_rewards.std()\n    return [(discounted_rewards - reward_mean) / reward_std\n            for discounted_rewards in all_discounted_rewards]\n\nn_iterations = 200\nn_episodes_per_update = 16\nn_max_steps = 1000\ndiscount_rate = 0.99\n\noptimizer = keras.optimizers.Nadam(learning_rate=0.005)\nloss_fn = keras.losses.sparse_categorical_crossentropy\n\nenv.seed(42)\n\nmean_rewards = []\n\nfor iteration in range(n_iterations):\n    all_rewards, all_grads = lander_play_multiple_episodes(\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n    print(\"\\rIteration: {}/{}, mean reward: {:.1f}  \".format(\n        iteration + 1, n_iterations, mean_reward), end=\"\")\n    mean_rewards.append(mean_reward)\n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n                                                       discount_rate)\n    all_mean_grads = []\n    for var_index in range(len(model.trainable_variables)):\n        mean_grads = tf.reduce_mean(\n            [final_reward * all_grads[episode_index][step][var_index]\n             for episode_index, final_rewards in enumerate(all_final_rewards)\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\n        all_mean_grads.append(mean_grads)\n    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n\nimport matplotlib.pyplot as plt\n\nplt.plot(mean_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean reward\")\nplt.grid()\nplt.show()\n\ndef lander_render_policy_net(model, n_max_steps=500, seed=42):\n    frames = []\n    env = gym.make(\"LunarLander-v2\")\n    env.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    obs = env.reset()\n    for step in range(n_max_steps):\n        frames.append(env.render(mode=\"rgb_array\"))\n        probas = model(obs[np.newaxis])\n        logits = tf.math.log(probas + keras.backend.epsilon())\n        action = tf.random.categorical(logits, num_samples=1)\n        obs, reward, done, info = env.step(action[0, 0].numpy())\n        if done:\n            break\n    env.close()\n    return frames\n\nframes = lander_render_policy_net(model, seed=42)\nplot_animation(frames) END"}
{"prompt": "TF-Agents\n\nQuestion: Use TF-Agents to train an agent that can achieve a superhuman level at SpaceInvaders-v4 using any of the available algorithms.\n\nCode:", "completion": " class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n    def skip_frames(self, num_skip):\n        for _ in range(num_skip):\n          super().step(0) # NOOP for num_skip steps\n    def reset(self, **kwargs):\n        obs = super().reset(**kwargs)\n        self.skip_frames(40)\n        return obs\n    def step(self, action):\n        lives_before_action = self.ale.lives()\n        obs, rewards, done, info = super().step(action)\n        if self.ale.lives() < lives_before_action and not done:\n            self.skip_frames(40)\n        return obs, rewards, done, info END"}
