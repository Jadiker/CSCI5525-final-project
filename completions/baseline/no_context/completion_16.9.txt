

import tensorflow as tf import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.preprocessing import LabelEncoder from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, LSTM, Dense from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.utils import plot_model np.random.seed(42) tf.random.set_seed(42) # Generate a synthetic dataset def generate_dataset(m=10000): # Convert dates from one format to another format_1 = ['%d-%m-%Y', '%d/%m/%Y', '%d%m%Y', '%d * %m * %Y'] format_2 = ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', '%Y * %m * %d'] # Generate dates in one format dates_1 = np.array([], dtype=np.str) for i in range(m): d1 = np.random.choice(format_1) dates_1 = np.append(dates_1, np.random.choice(pd.date_range(start='1/1/1980', end='1/1/2020').strftime(d1))) # Convert dates to another format dates_2 = np.array([], dtype=np.str) for date in dates_1: d2 = np.random.choice(format_2) dates_2 = np.append(dates_2, pd.to_datetime(date).strftime(d2)) return shuffle(dates_1, dates_2, random_state=42) # Convert a string to a one-hot encoded vector def one_hot_encode(s, n_unique_chars): # Create an empty vector encoding = np.zeros((len(s), n_unique_chars)) # Loop over the characters in the string for i, c in enumerate(s): # Get the index of the character encoding[i, c] = 1 return encoding # Define the model def create_model(n_unique_chars, hidden_dim): # Define the input layer input_layer = Input(shape=(None, n_unique_chars)) # Define the encoder LSTM layer encoder_layer = LSTM(hidden_dim, return_state=True) encoder_outputs, state_h, state_c = encoder_layer(input_layer) # We discard `encoder_outputs` and only keep the final states. encoder_states = [state_h, state_c] # Define the decoder LSTM layer decoder_layer = LSTM(hidden_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_layer(input_layer, initial_state=encoder_states) # Define the output layer output_layer = Dense(n_unique_chars, activation='softmax') output = output_layer(decoder_outputs) # Define the model model = Model(input_layer, output) # Compile the model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model # Generate the dataset X, y = generate_dataset() # Convert the input and output strings to one-hot encoded vectors X_encoded = one_hot_encode(X, len(np.unique(np.array(list(X))))) y_encoded = one_hot_encode(y, len(np.unique(np.array(list(y))))) # Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.1) # Define the model model = create_model(len(np.unique(np.array(list(X)))), hidden_dim=256) # Train the model model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=100)